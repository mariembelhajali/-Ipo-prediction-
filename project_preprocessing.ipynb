{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFB Course Project: Predicting IPO Share Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg)\n",
    "\n",
    "image source : https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Initial Public Offering (IPO) is the process by which a private company becomes publicly traded on a stock exchange. The IPO company offers its shares to public investors in exchange of capital for sustaining expansion and growth. For this reason, IPOs are often issued by small or young companies, but they can also be done by large  companies looking to become publicly traded. During an IPO, the company obtains the assistance of an investment bank (underwriter), which helps determine the type, amount and price of the shares being offered. Decisions about the offering price are particularly important to avoid incurring excessive costs and maximize the capital received in the IPO. However at the end of the first trading day, price of each share can change due to market dynamics, which can lead to a price higher or lower than the offering one.\n",
    "\n",
    "During an Initial Public Offering (IPO), the firm’s management have to disclose all relevant information about their business in a filing with the government called the \"IPO Prospectus.\" Although there might be concerns about the public disclosure of sensitive information in the Prospectus that can help competitors, firms are encouraged to be as transparent as possible in order to avoid future litigation (lawsuits). A key textual field from the prospectus is:\n",
    "\n",
    "__Risk_Factors__: Firms have to disclose all relevant information about internal or external risk factors that might affect future business performances. This information is contained in the “Risk Factors” section of the IPO prospectus. \n",
    "\n",
    "The key pricing variables are:\n",
    "\n",
    "__Offering_Price__: the price at which a company sells its shares to investors.\n",
    "\n",
    "__Num_Shares__: the total number of outstanding shares.\n",
    "\n",
    "__Closing_Price__: (at the end of the first day of training) price at which shares trade in the open market, measured at the end of the first day of trading.\n",
    "\n",
    "In this project you are provided with IPO data of different firms that are collected from different sources. You can find the dataset under project directory in the course git repository under the name of *ipo.xlsx*. The description of other variables can be found in *variable_description.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will be presented as follow :\n",
    "\n",
    "# Pre Processing\n",
    "#####  libraries & useful functions\n",
    "## 1 Import Dataset\n",
    "## 2 Pre-processing of Dates Features\n",
    "## 3 Pre-processsing of Special Features\n",
    "## 4 Pre-processing of Categorical Features\n",
    "## 5 Pre-processing of Ordinal Features\n",
    "## 6 Pre-processing of Numerical Features\n",
    "## 7 Feature Reduction\n",
    "        1 Correlation with target\n",
    "        2 Correlation between features\n",
    "## 8 Send CSV      \n",
    "# -----------------------------------------\n",
    "## 9 Pre-processing of Textual Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "##### Libraries & Useful functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd     \n",
    "import seaborn as sns\n",
    "import re as re\n",
    "import nltk\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import roc_curve\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import Dataset \n",
    "\n",
    "##### Import,  explore dataset and check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "DATA_FOLDER = 'data'\n",
    "ipo = pd.read_excel(DATA_FOLDER + '/ipo.xlsx')\n",
    "ipo_to_predict = pd.read_excel(DATA_FOLDER+'/ipo_to_predict.xlsx')\n",
    "\n",
    "#merge\n",
    "ipo = ipo.append(ipo_to_predict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaration of categorization table to use in the data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = ['amd_date', 'lockup_date', 'lockup_days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ordinal Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_col = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = ['amd_nbr','round_tot','mgt_fee', 'gross_spread', 'min_round_vexp','avg_round_vexp', 'max_round_vexp',\\\n",
    "           'min_firm_amt_vexp', 'avg_firm_amt_vexp', 'max_firm_amt_vexp',\\\n",
    "          'min_fund_amt_vexp', 'max_fund_amt_vexp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['exch', 'mgrs_role', 'mgrs','all_sic' ,'description','ht_ind', 'ht_ind_gr', 'ind_group','legal', 'uop' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columns to Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ID','dj_avg_2','nasdaq_avg_2','SP2','SP4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if duplicates\n",
    "ipo[ipo.astype(str).duplicated()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of actual features\n",
    "len(ipo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "nb_missing_values = sum(map(any, ipo.isnull()))\n",
    "nb_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count missing \n",
    "features_with_nan = len(ipo) - ipo.count()\n",
    "features_with_nan=features_with_nan[features_with_nan!=0]\n",
    "nan_percentage=(features_with_nan/len(ipo)).to_frame('percentage').reset_index().rename(columns={'index':'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nan_percentage = nan_percentage[nan_percentage.percentage>0.5]\n",
    "features_nan_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop column with more than 50% of missing values\n",
    "list_to_append = features_nan_percentage.feature.values\n",
    "columns_to_drop = add_list_to_list(list_to_append, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check type of values\n",
    "ipo.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devide dataframe in function of type.\n",
    "ints = ipo.select_dtypes(include='int64')\n",
    "floats = ipo.select_dtypes(include='float64')\n",
    "objects = ipo.select_dtypes(include='object')\n",
    "timestamps = ipo.select_dtypes(include='M8[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the df\n",
    "ipo_processing = ipo.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pre-processsing of Date Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amd_date\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate number of amendment\n",
    "ipo_processing['amd_nbr'] = ipo['amd_date'].apply(lambda x:  str(x).count('\\n')+1 if( str(x).count('\\n') > 0) else ( 1 if(len(str(x)) > 0) else 0 ) )\n",
    "num_col.append('amd_nbr')\n",
    "columns_to_drop.append('amd_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lockup_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if missing values are the same\n",
    "sns.heatmap(ipo[['lockup_date','lockup_days', ]].isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get days only last \n",
    "ipo_processing['lockup_last_duration'] = ipo['lockup_days'].apply(lambda x: str(x) if( pd.isnull(x) ) else str(x)[-3:] )\n",
    "#Cast it as a number\n",
    "ipo_processing['lockup_last_duration'] = ipo_processing['lockup_last_duration'].apply(lambda x: int(x) if( str(x).isdigit() ) else np.nan )\n",
    "#replace missing values\n",
    "ipo_processing['lockup_last_duration'] = replace_nan(ipo_processing['lockup_last_duration'], '')\n",
    "\n",
    "#Add\n",
    "num_col.append('lockup_last_duration')\n",
    "\n",
    "#We remove lockup_date because this information is rundandant in the lockup days we can calculate from the frist trade date\n",
    "columns_to_drop.append('lockup_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### offer_date, first_trade_date, issue_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare number of missing value offer_date and issue_date ares nearly the same.\n",
    "sns.heatmap(ipo[['issue_date','offer_date', 'first_trade_date']].isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing value by the most frequent one and get timestamp to compare datas\n",
    "for columnWeTransform in ['issue_date','offer_date', 'first_trade_date']:\n",
    "    ipo_processing[columnWeTransform] = ipo[columnWeTransform].apply(lambda x: str(x)[0:10])\n",
    "    ipo_processing[columnWeTransform] = replace_nan(ipo[columnWeTransform], '1900-01-01')\n",
    "    ipo_processing[columnWeTransform] = ipo_processing[columnWeTransform].apply(lambda x: pd.to_datetime(x).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the values\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(ipo_processing[['issue_date','offer_date', 'first_trade_date']], cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop first_trade_date and issue_date and keep offer_date\n",
    "columns_to_drop.append('first_trade_date')\n",
    "columns_to_drop.append('issue_date')\n",
    "columnWeTransform = 'offer_date'\n",
    "\n",
    "# read dataset of the american 10 year bond rate\n",
    "# This give us more info on the actual risk premium\n",
    "rate = pd.read_csv(DATA_FOLDER + '/HQMCB10YR.csv')\n",
    "rate.rename(columns = {'DATE':columnWeTransform, 'HQMCB10YR':'rate'}, inplace = True)\n",
    "rate[columnWeTransform] = rate[columnWeTransform].apply(lambda x: pd.to_datetime(x))\n",
    "rate.head(2)\n",
    "\n",
    "#Merge rate on the dataframe\n",
    "ipo_processing[columnWeTransform] = ipo[columnWeTransform].apply(lambda x: pd.to_datetime(str(x.year)+'-'+str(x.month)+'-01') if( not(pd.isna(x))) else pd.to_datetime('1900-01-01'))\n",
    "ipo_processing = ipo_processing.merge(rate, how='left')\n",
    "columns_to_drop.append(columnWeTransform)\n",
    "num_col.append('rate')\n",
    "\n",
    "#Replace missing value rate\n",
    "ipo_processing.rate = replace_nan(ipo_processing.rate, '')\n",
    "\n",
    "#Replace missing value by the most frequent one.\n",
    "ipo_processing[columnWeTransform] = ipo[columnWeTransform].apply(lambda x: str(x)[0:10])\n",
    "ipo_processing[columnWeTransform] = replace_nan(ipo[columnWeTransform], '1900-01-01')\n",
    "ipo_processing[columnWeTransform] = ipo_processing[columnWeTransform].apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "#Create new features\n",
    "# Timestamp\n",
    "ipo_processing['timestamp'] = ipo_processing[columnWeTransform].apply(lambda x: dt.datetime(year=int(x.year), month=int(x.month), day=int(x.day)).timestamp())\n",
    "num_col.append('timestamp')\n",
    "# Year\n",
    "ipo_processing['year'] = ipo_processing[columnWeTransform].apply(lambda x: x.year)\n",
    "num_col.append('year')\n",
    "\n",
    "# Convert circular temporal features into sin and cos features\n",
    "# Month\n",
    "ipo_processing['month'] = ipo_processing[columnWeTransform].apply(lambda x: x.month)\n",
    "ipo_processing['month_sin'] = np.sin((ipo_processing.month-1)*(2.*np.pi/12))\n",
    "ipo_processing['month_cos'] = np.cos((ipo_processing.month-1)*(2.*np.pi/12))\n",
    "columns_to_drop.append('month')\n",
    "num_col.append('month_sin')\n",
    "num_col.append('month_cos')\n",
    "# Day\n",
    "ipo_processing['day'] = ipo_processing[columnWeTransform].apply(lambda x: x.day)\n",
    "ipo_processing['day_sin'] = np.sin((ipo_processing.day-1)*(2.*np.pi/31))\n",
    "ipo_processing['day_cos'] = np.cos((ipo_processing.day-1)*(2.*np.pi/31))\n",
    "columns_to_drop.append('day')\n",
    "num_col.append('day_sin')\n",
    "num_col.append('day_cos')\n",
    "# Day of week\n",
    "ipo_processing['dow'] = ipo_processing[columnWeTransform].apply(lambda x: x.dayofweek)\n",
    "ipo_processing['dow_sin'] = np.sin((ipo_processing.dow-1)*(2.*np.pi/7))\n",
    "ipo_processing['dow_cos'] = np.cos((ipo_processing.dow-1)*(2.*np.pi/7))\n",
    "columns_to_drop.append('dow')\n",
    "num_col.append('dow_sin')\n",
    "num_col.append('dow_cos')\n",
    "\n",
    "columns_to_drop.append(columnWeTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date_amd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from excel date to datetime\n",
    "ipo_processing['date_amd'] = replace_nan(ipo['date_amd'], '')\n",
    "ipo_processing['date_amd'] = ipo_processing['date_amd'].apply(lambda x: from_excel_ordinal(x) )\n",
    "\n",
    "#Calculate difference between date_amd and offer_date save number of days as int\n",
    "ipo_processing['offer_date'] = ipo['offer_date'].apply(lambda x: pd.to_datetime(x) )\n",
    "ipo_processing['time_amd'] = ipo_processing.apply(lambda x: x['offer_date']-x['date_amd'], axis=1 )\n",
    "ipo_processing['time_amd'] = ipo_processing['time_amd'].apply(lambda x: x.days)\n",
    "num_col.append('time_amd')\n",
    "\n",
    "#column to drop:\n",
    "columns_to_drop.append('date_amd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Pre-processsing of Special Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split it into 2 one numerical and second categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo['description'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the ',' in the features description\n",
    "ipo_processing['description'] = ipo['description'].str.replace(',','',regex=False)\n",
    "\n",
    "# split description into 2 string\n",
    "split_description = ipo_processing.description.str.split('.0 ')\n",
    "\n",
    "# create desctiption_numeric as the nb written in description \n",
    "ipo_processing['description_numeric']= split_description.apply(lambda x: int(x[0]))\n",
    "num_col.append('description_numeric')\n",
    "\n",
    "# create description cat as the category of the number of shares\n",
    "ipo_processing['description_cat'] =  split_description.apply(lambda x: x[1])\n",
    "cat_col.append('description_cat')\n",
    "\n",
    "# drop description\n",
    "columns_to_drop.append('description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all_sic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reduce this number by a 2 digit categories for all_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.all_sic = ipo.all_sic.str.split('/')\n",
    "\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x: [a[:2] for a in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this file to reduce the number of categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sic_mapping = pd.read_excel(DATA_FOLDER + '/all_sic_mapping.xlsx',delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning file:\n",
    "all_sic_mapping.range_sic = all_sic_mapping.range_sic.str.split('-')\n",
    "all_sic_mapping.range_sic= all_sic_mapping.range_sic.apply(lambda x: [int(a[:2]) for a in x])\n",
    "all_sic_mapping.range_sic = all_sic_mapping.range_sic.apply(lambda x: np.arange(x[0],x[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map all_sic values to new ones\n",
    "splitted = all_sic_mapping.range_sic.apply(pd.Series).stack().reset_index(level = 1,drop = True).to_frame('range')\n",
    "merged = pd.merge(splitted,all_sic_mapping,left_on = splitted.index, right_on=  all_sic_mapping.index,how = 'left')\n",
    "merged_2 = merged[['range','cat']]\n",
    "merged_2.range = merged_2.range.astype(int)\n",
    "merged_2.set_index('range',inplace = True)\n",
    "\n",
    "#Apply to the dataset\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x : [merged_2.loc[int(a)] if (int(a) in merged_2.index.values) else int(a) for a in x ] )\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing = pd.concat([ipo_processing,pd.get_dummies(ipo_processing.all_sic.apply(pd.Series).stack()).sum(level=0)],axis = 1)\n",
    "\n",
    "#Add new categorical col\n",
    "cat_col = add_list_to_list(all_sic_mapping.cat, cat_col)\n",
    "\n",
    "#Columns to drop\n",
    "columns_to_drop.append('all_sic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip\n",
    "We will use this file 'us_postal_codes.csv' to get cities and state to replace where there is missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read us postal file\n",
    "us_zip = pd.read_csv(DATA_FOLDER + '/us_postal_codes.csv', sep=',')\n",
    "us_zip.rename(columns={'Zip Code': 'zip', 'Place Name':'city_zip', 'State':'state_zip'}, inplace=True)\n",
    "\n",
    "# Drop infos that we don't want\n",
    "us_zip = us_zip.drop(columns=['State Abbreviation', 'County', 'Latitude', 'Longitude'])\n",
    "us_zip.head()\n",
    "\n",
    "#Merge in function of the ZIP code\n",
    "ipo_processing.zip = ipo.zip.apply(lambda x: str(x)[:5])\n",
    "ipo_processing.zip = ipo_processing.zip.apply(lambda x: int(x) if (str.isdigit(x)) else 0 )\n",
    "ipo_processing['zip'] = ipo_processing.apply(lambda x: int(str(x['zip'])[:5]) if ( str(x['nation']) == 'United States' ) else 0 , axis=1)\n",
    "ipo_processing = ipo_processing.merge(us_zip, how='left')\n",
    "ipo_processing.state.fillna('Outside US', inplace=True)\n",
    "\n",
    "#Replace where missing city and stateS\n",
    "print(\"Number of city replaced : \" + str(ipo_processing['city'].isna().sum() - ipo_processing['city_zip'].loc[ipo_processing['city'].isna()].isna().sum()))\n",
    "ipo_processing['city'].loc[ipo_processing['city'].isna()] = ipo_processing.loc[ipo_processing['city'].isna()]['city_zip']\n",
    "\n",
    "print(\"Number of state replaced : \" + str(ipo_processing['state'].isna().sum() - ipo_processing['state_zip'].loc[ipo_processing['state'].isna()].isna().sum()))\n",
    "ipo_processing['state'].loc[ipo_processing['state'].isna()] = ipo_processing.loc[ipo_processing['state'].isna()]['state_zip']\n",
    "\n",
    "#Remove city_zip and zip\n",
    "columns_to_drop.append('state_zip')\n",
    "columns_to_drop.append('city_zip')\n",
    "columns_to_drop.append('zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Pre-processing of Categorical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### issuer\n",
    "\n",
    "Issuer is not relevant because has same number of issuer than number of row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo['issuer']) == len(ipo.issuer.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop.append('issuer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing categories with \\n separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_n = ['mgrs_role','mgrs','ht_ind_gr','ht_ind','uop','legal','exch','br']\n",
    "\n",
    "for col in columns_with_n:\n",
    "    print('processing column ' + col)\n",
    "    ipo_processing,columns_to_drop = process_categorical_with_sep(ipo_processing,col,'\\n',columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing categories with / separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_I = ['industry']\n",
    "\n",
    "for col in columns_with_I:\n",
    "    print('processing column ' + col)\n",
    "    ipo_processing,columns_to_drop = process_categorical_with_sep(ipo_processing,col,'/',columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing all others categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dummies = ['description_cat','state','auditor','city','ind_group','veic_descr','naic_primary','naic_decr','public_descr','lockup_flag','sic_main','nation','lbo','prim_naic','prim_uop','pe_backed','shs_out_after','vc']\n",
    "to_dummies = add_list_to_list(cat_col, to_dummies)\n",
    "to_dummies = list(set(to_dummies))\n",
    "\n",
    "for col in to_dummies:\n",
    "    if not col in all_sic_mapping['cat'].tolist():\n",
    "        if not col in columns_to_drop:\n",
    "            print('processing column ' + col)\n",
    "            ipo_processing,columns_to_drop = process_cat_columns(ipo_processing,col,columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Pre-processing of Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price_range : Ordinal \n",
    "Since Price_range is an ordinal feature we will map it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_col = ['price_range']\n",
    "\n",
    "ipo_processing.price_range.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_range = ipo['price_range']\n",
    "\n",
    "print(len(price_range.unique()))\n",
    "price_range = replace_nan(price_range,'nan')\n",
    "print(len(price_range.unique()))\n",
    "\n",
    "dict_p = {'Above range': 3,\n",
    "          'Within range':2,\n",
    "          'Below range':1, \n",
    "            np.nan: 0}\n",
    "\n",
    "ipo_processing.price_range = ipo_processing.price_range.replace(dict_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offering_Price: the price at which a company sells its shares to investors.\n",
    "\n",
    "Num_Shares: the total number of outstanding shares.\n",
    "\n",
    "Closing_Price: (at the end of the first day of training) price at which shares trade in the open market, measured at the end of the first day of trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Pre-processing of Numerical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by numerical features that needs more preprocessing, and then replace all nan values at the end in all numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### round_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum value\n",
    "ipo_processing['round_tot'] = ipo['round_tot']\n",
    "ipo_processing = sum_list_columns(ipo_processing,'round_tot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lockup_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum days\n",
    "ipo_processing['lockup_days'] = ipo['lockup_days']\n",
    "ipo_processing = sum_list_columns(ipo_processing,'lockup_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mgt_fee, gross_spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace what is not numeric by a nan will replace all nan at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing['mgt_fee'] =ipo['mgt_fee'].replace('Comb.', np.nan)\n",
    "ipo_processing['gross_spread'] = ipo['gross_spread'].replace('na', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace ' in some numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_replace = ['min_round_vexp','avg_round_vexp', 'max_round_vexp', 'min_firm_amt_vexp', 'avg_firm_amt_vexp', 'max_firm_amt_vexp',\\\n",
    "          'min_fund_amt_vexp', 'max_fund_amt_vexp']\n",
    "for col in col_to_replace:\n",
    "    ipo_processing[col] = ipo[col].str.replace(\"'\",'')\n",
    "num_col = add_list_to_list(col_to_replace, num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing values of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###  Replace missing values in all num\n",
    "for col in num_col:\n",
    "    if not(col in columns_to_drop):\n",
    "        ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace missing values in all float\n",
    "floats = ipo.select_dtypes(include='float64')\n",
    "\n",
    "for col in floats.columns:\n",
    "    if not(col in columns_to_drop):\n",
    "        ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace missing values in all float\n",
    "ints = ipo.select_dtypes(include='int64')\n",
    "\n",
    "for col in ints.columns:\n",
    "    if not(col in columns_to_drop):\n",
    "        ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.int32))[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final check for the Nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns_to_drop)\n",
    "ipo_processing.drop(columns_to_drop,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.columns[ipo_processing.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Check the correlation between feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_dic = {}\n",
    "for column in ipo_processing.columns:\n",
    "    corr_dic[column] = abs(ipo_processing[column]\n",
    "                           .corr(ipo_processing['Target']))\n",
    "\n",
    "for w in sorted(corr_dic, key=corr_dic.get, reverse=True):\n",
    "    print (w, corr_dic[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost TIME\n",
    "ipo_processing[0:3000].to_csv(DATA_FOLDER+'/data_non_textual_clean.csv')\n",
    "ipo_processing[3000:].to_csv(DATA_FOLDER+'/data_non_textual_clean_predict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 TEXT Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process text COST TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cost time !\n",
    "'''\n",
    "risks,risks_words = process_text_columns(ipo['Risk_Factors'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 500)\n",
    "risks_tfidf = vectorizer.fit_transform(risks)\n",
    "risks_tfidf = risks_tfidf.toarray()\n",
    "\n",
    "#Convert to a dataframe\n",
    "risks_tfidf = pd.DataFrame(risks_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks_tfidf[0:3000].to_csv(DATA_FOLDER+'/data_textual_clean.csv')\n",
    "risks_tfidf[3000:].to_csv(DATA_FOLDER+'/data_textual_clean_predict.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
