{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFB Course Project: Predicting IPO Share Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg)\n",
    "\n",
    "image source : https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Initial Public Offering (IPO) is the process by which a private company becomes publicly traded on a stock exchange. The IPO company offers its shares to public investors in exchange of capital for sustaining expansion and growth. For this reason, IPOs are often issued by small or young companies, but they can also be done by large  companies looking to become publicly traded. During an IPO, the company obtains the assistance of an investment bank (underwriter), which helps determine the type, amount and price of the shares being offered. Decisions about the offering price are particularly important to avoid incurring excessive costs and maximize the capital received in the IPO. However at the end of the first trading day, price of each share can change due to market dynamics, which can lead to a price higher or lower than the offering one.\n",
    "\n",
    "During an Initial Public Offering (IPO), the firm’s management have to disclose all relevant information about their business in a filing with the government called the \"IPO Prospectus.\" Although there might be concerns about the public disclosure of sensitive information in the Prospectus that can help competitors, firms are encouraged to be as transparent as possible in order to avoid future litigation (lawsuits). A key textual field from the prospectus is:\n",
    "\n",
    "__Risk_Factors__: Firms have to disclose all relevant information about internal or external risk factors that might affect future business performances. This information is contained in the “Risk Factors” section of the IPO prospectus. \n",
    "\n",
    "The key pricing variables are:\n",
    "\n",
    "__Offering_Price__: the price at which a company sells its shares to investors.\n",
    "\n",
    "__Num_Shares__: the total number of outstanding shares.\n",
    "\n",
    "__Closing_Price__: (at the end of the first day of training) price at which shares trade in the open market, measured at the end of the first day of trading.\n",
    "\n",
    "In this project you are provided with IPO data of different firms that are collected from different sources. You can find the dataset under project directory in the course git repository under the name of *ipo.xlsx*. The description of other variables can be found in *variable_description.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will be presented as follow :\n",
    "\n",
    "# Part 0 : Pre Processing\n",
    "#####  libraries & useful functions\n",
    "### 0.1 Import Dataset\n",
    "### 0.2 Pre-processsing of Special Features\n",
    "### 0.3 Pre-processing of Categorical Features\n",
    "### 0.4 Pre-processing of Ordinal Features\n",
    "### 0.5 Pre-processing of Dates Features\n",
    "### 0.6 Pre-processing of Numerical Features\n",
    "### 0.7 Pre-processing of Textual Features\n",
    "### 0.8 Target\n",
    "### 0.9 Feature Reduction\n",
    "        1 Correlation with target\n",
    "        2 Correlation between features\n",
    "# Part 1 :\n",
    "# Part 2 :\n",
    "# Part 3 :\n",
    "# Part 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 : Pre-Processing\n",
    "\n",
    "##### Libraries & Useful functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import re as re\n",
    "import nltk\n",
    "import datetime as dt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import roc_curve\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import Dataset \n",
    "\n",
    "##### Import,  explore dataset and check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "DATA_FOLDER = 'data'\n",
    "ipo = pd.read_excel(DATA_FOLDER + '/ipo.xlsx')\n",
    "ipo_to_predict = pd.read_excel(DATA_FOLDER+'/ipo_to_predict.xlsx')\n",
    "\n",
    "#merge\n",
    "ipo = ipo.append(ipo_to_predict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaration of categorization table to use in the data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = ['amd_nbr','round_tot','mgt_fee', 'gross_spread', 'min_round_vexp','avg_round_vexp', 'max_round_vexp',\\\n",
    "           'min_firm_amt_vexp', 'avg_firm_amt_vexp', 'max_firm_amt_vexp',\\\n",
    "          'min_fund_amt_vexp', 'max_fund_amt_vexp','description_numeric', 'rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['exch', 'mgrs_role', 'mgrs','all_sic','auditor', 'city','description','ht_ind', 'ht_ind_gr','industry', 'ind_group', 'issuer',\\\n",
    "          'veic_descr','legal', 'naic_primary','naic_decr', 'public_descr', 'lockup_flag','sic_main', 'nation', 'lbo', 'prim_naic', 'prim_uop', 'pe_backed', 'shs_out_after',\\\n",
    "          'state', 'uop', 'vc', 'zip','description_cat' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columns to Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "columns_to_drop.append('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if duplicates\n",
    "ipo[ipo.astype(str).duplicated()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of actual features\n",
    "len(ipo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "nb_missing_values = sum(map(any, ipo.isnull()))\n",
    "nb_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count missing \n",
    "features_with_nan = len(ipo) - ipo.count()\n",
    "features_with_nan=features_with_nan[features_with_nan!=0]\n",
    "nan_percentage=(features_with_nan/len(ipo)).to_frame('percentage').reset_index().rename(columns={'index':'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nan_percentage = nan_percentage[nan_percentage.percentage>0.5]\n",
    "features_nan_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop column with more than 80% of missing values\n",
    "list_to_append = features_nan_percentage.feature.values\n",
    "for col in list_to_append:\n",
    "    columns_to_drop.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check type of values\n",
    "ipo.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devide dataframe in function of type.\n",
    "ints = ipo.select_dtypes(include='int64')\n",
    "floats = ipo.select_dtypes(include='float64')\n",
    "objects = ipo.select_dtypes(include='object')\n",
    "timestamps = ipo.select_dtypes(include='M8[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = ['amd_date', 'lockup_date', 'lockup_days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Pre-processsing of Special Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the df\n",
    "ipo_processing = ipo.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split it into 2 one numerical and second categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing['description'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the ',' in the features description\n",
    "ipo_processing['description'] = ipo['description'].str.replace(',','',regex=False)\n",
    "# split description into 2 string\n",
    "split_description = ipo_processing.description.str.split('.0 ')\n",
    "# create desctiption_numeric as the nb written in description \n",
    "ipo_processing['description_numeric']= split_description.apply(lambda x: int(x[0]))\n",
    "# create description cat as the category of the number of shares\n",
    "ipo_processing['description_cat'] =  split_description.apply(lambda x: x[1])\n",
    "\n",
    "ipo_processing['description_numeric'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop description\n",
    "columns_to_drop.append('description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all_sic : COMMENT THE CELL BY EXPLAINING WHATS HAPPENING INSIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.all_sic.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to reduce this number by a 2 digit categories for all_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.all_sic = ipo.all_sic.str.split('/')\n",
    "\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x: [a[:2] for a in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this file to reduce the number of categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sic_mapping = pd.read_excel(DATA_FOLDER + '/all_sic_mapping.xlsx',delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sic_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning file:\n",
    "all_sic_mapping.range_sic = all_sic_mapping.range_sic.str.split('-')\n",
    "\n",
    "all_sic_mapping.range_sic= all_sic_mapping.range_sic.apply(lambda x: [int(a[:2]) for a in x])\n",
    "\n",
    "all_sic_mapping.range_sic = all_sic_mapping.range_sic.apply(lambda x: np.arange(x[0],x[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sic_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map all_sic values to new ones\n",
    "splitted = all_sic_mapping.range_sic.apply(pd.Series).stack().reset_index(level = 1,drop = True).to_frame('range')\n",
    "\n",
    "merged = pd.merge(splitted,all_sic_mapping,left_on = splitted.index, right_on=  all_sic_mapping.index,how = 'left')\n",
    "\n",
    "merged_2 = merged[['range','cat']]\n",
    "\n",
    "merged_2.range = merged_2.range.astype(int)\n",
    "\n",
    "merged_2.set_index('range',inplace = True)\n",
    "\n",
    "\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x : [merged_2.loc[int(a)] if (int(a) in merged_2.index.values) else int(a) for a in x ] )\n",
    "\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing = pd.concat([ipo_processing,pd.get_dummies(ipo_processing.all_sic.apply(pd.Series).stack()).sum(level=0)],axis = 1)\n",
    "columns_to_drop.append('all_sic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip : done\n",
    "We will use this file 'us_postal_codes.csv' to get cities and state to replace where there is missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read us postal file\n",
    "us_zip = pd.read_csv(DATA_FOLDER + '/us_postal_codes.csv', sep=',')\n",
    "us_zip.rename(columns={'Zip Code': 'zip', 'Place Name':'city_zip', 'State':'state_zip'}, inplace=True)\n",
    "\n",
    "# Drop infos that we don't want\n",
    "us_zip = us_zip.drop(columns=['State Abbreviation', 'County', 'Latitude', 'Longitude'])\n",
    "us_zip.head()\n",
    "\n",
    "#Merge State in function of the ZIP code\n",
    "ipo_processing.zip = ipo.zip.apply(lambda x: str(x)[:5])\n",
    "ipo_processing.zip = ipo_processing.zip.apply(lambda x: int(x) if (str.isdigit(x)) else 0 )\n",
    "ipo_processing['zip'] = ipo_processing.apply(lambda x: int(str(x['zip'])[:5]) if ( str(x['nation']) == 'United States' ) else 0 , axis=1)\n",
    "ipo_processing = ipo_processing.merge(us_zip, how='left')\n",
    "ipo_processing.state.fillna('Outside US', inplace=True)\n",
    "\n",
    "#Replace where missing city and state\n",
    "ipo_processing['city'].loc[ipo_processing['city'].isna()] = ipo_processing.loc[ipo_processing['city'].isna()]['city_zip']\n",
    "ipo_processing['state'].loc[ipo_processing['state'].isna()] = ipo_processing.loc[ipo_processing['state'].isna()]['state_zip']\n",
    "\n",
    "#Remove city_zip and zip\n",
    "columns_to_drop.append('state_zip')\n",
    "columns_to_drop.append('city_zip')\n",
    "columns_to_drop.append('zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Pre-processing of Categorical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### issuer : done DROP ABoVE ???\n",
    "\n",
    "issuer not relevant : ipo.naic_primary contient des fois des lettres? a supprimé?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo['issuer']) == len(ipo.issuer.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop.append('issuer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ipo_1.naic_primary BBBBBA  contient des caractères je ne sais psa si c'est faux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_dummies = ['description_cat','state','auditor','city','ind_group','veic_descr','naic_primary','naic_decr','public_descr','lockup_flag','sic_main','nation','lbo','prim_naic','prim_uop','pe_backed','shs_out_after','vc']\n",
    "\n",
    "for col in to_dummies:\n",
    "    print('processing column ' + col)\n",
    "    ipo_processing,columns_to_drop = process_cat_columns(ipo_processing,col,columns_to_drop)\n",
    "\n",
    "columns_with_n = ['mgrs_role','mgrs','ht_ind_gr','ht_ind','uop','legal','exch','br']\n",
    "for col in columns_with_n:\n",
    "    print('processing column ' + col)\n",
    "    ipo_processing,columns_to_drop = process_categorical_with_sep(ipo_processing,col,'\\n',columns_to_drop)\n",
    "\n",
    "print('Processing column industry')\n",
    "ipo_processing,columns_to_drop = process_categorical_with_sep(ipo_processing,'industry','/',columns_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Pre-processing of Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price_range : Ordinal \n",
    "Since Price_range is an ordinal feature we will map it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_col = ['price_range']\n",
    "\n",
    "ipo_processing.price_range.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_range = ipo['price_range']\n",
    "\n",
    "print(len(price_range.unique()))\n",
    "price_range = replace_nan(price_range,'nan')\n",
    "print(len(price_range.unique()))\n",
    "\n",
    "dict_p = {'Above range': 3,\n",
    "          'Within range':2,\n",
    "          'Below range':1, \n",
    "            np.nan: 0}\n",
    "\n",
    "ipo_processing.price_range = ipo_processing.price_range.replace(dict_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Pre-processing of Dates Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amd_date : Done\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate number of amendment\n",
    "ipo_processing['amd_nbr'] = ipo_processing['amd_date'].apply(lambda x:  str(x).count('\\n')+1 if( str(x).count('\\n') > 0) else ( 1 if(len(str(x)) > 0) else 0 ) )\n",
    "num_col.append('amd_nbr')\n",
    "columns_to_drop.append('amd_nbr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lockup_date, lockup_days : Done\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of missing values\n",
    "print(ipo['lockup_days'].isna().sum())\n",
    "print(ipo['lockup_date'].isna().sum())\n",
    "\n",
    "#Check if missing values are the same\n",
    "print(ipo.loc[ipo['lockup_days'].isna()].index)\n",
    "print(ipo.loc[ipo['lockup_date'].isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get days only last \n",
    "ipo_processing['lockup_days'] = ipo['lockup_days'].apply(lambda x: str(x) if( pd.isnull(x) ) else str(x)[-3:] )\n",
    "#Cast it as a number\n",
    "ipo_processing['lockup_days'] = ipo_processing['lockup_days'].apply(lambda x: int(x) if( str(x).isdigit() ) else np.nan )\n",
    "#replace missing values\n",
    "ipo_processing['lockup_days'] = replace_nan(ipo_processing['lockup_days'], '')\n",
    "\n",
    "#ipo_processing['lockup_days'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove lockup_date because this information is rundandant in the lockup days we can calculate from the frist trade date\n",
    "columns_to_drop.append('lockup_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### offer_date, first_trade_date, issue_date : DONE\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare number of missing value offer_date and issue_date ares nearly the same.\n",
    "print(ipo['issue_date'].isna().sum() )\n",
    "print(ipo['offer_date'].isna().sum() )\n",
    "print(ipo['first_trade_date'].isna().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop first_trade_date\n",
    "columns_to_drop.append('first_trade_date')\n",
    "columns_to_drop.append('issue_date')\n",
    "columnWeTransform = 'offer_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset of the american 10 year bond rate\n",
    "# This give us more info on the actual risk premium\n",
    "rate = pd.read_csv(DATA_FOLDER + '/HQMCB10YR.csv')\n",
    "rate.rename(columns = {'DATE':columnWeTransform, 'HQMCB10YR':'rate'}, inplace = True)\n",
    "rate[columnWeTransform] = rate[columnWeTransform].apply(lambda x: pd.to_datetime(x))\n",
    "rate.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge rate on the dataframe\n",
    "ipo_processing[columnWeTransform] = ipo[columnWeTransform].apply(lambda x: pd.to_datetime(str(x.year)+'-'+str(x.month)+'-01') if( not(pd.isna(x))) else pd.to_datetime('1900-01-01'))\n",
    "ipo_processing = ipo_processing.merge(rate, how='left')\n",
    "columns_to_drop.append(columnWeTransform)\n",
    "num_col.append('rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing value rate\n",
    "ipo_processing.rate = replace_nan(ipo_processing.rate, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing value by the most frequent one.\n",
    "ipo_processing[columnWeTransform] = ipo[columnWeTransform].apply(lambda x: str(x)[0:10])\n",
    "ipo_processing[columnWeTransform] = replace_nan(ipo[columnWeTransform], '1900-01-01')\n",
    "ipo_processing[columnWeTransform] = ipo_processing[columnWeTransform].apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get datetime object for each row\n",
    "ipo_processing[columnWeTransform+'_day'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.day) )\n",
    "cat_col.append(columnWeTransform+'_day')\n",
    "ipo_processing[columnWeTransform+'_weekday'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.weekday()) )\n",
    "cat_col.append(columnWeTransform+'_weekday')\n",
    "ipo_processing[columnWeTransform+'_month'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.month) )\n",
    "cat_col.append(columnWeTransform+'_month')\n",
    "ipo_processing[columnWeTransform+'_year'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.year) )\n",
    "cat_col.append(columnWeTransform+'_year')\n",
    "ipo_processing[columnWeTransform+'_timestamp'] = ipo_processing[columnWeTransform].apply(lambda x: dt.datetime(year=int(x.year), month=int(x.month), day=int(x.day)).timestamp())\n",
    "num_col.append(columnWeTransform+'_timestamp')\n",
    "\n",
    "#hot encode category\n",
    "ipo_processing = pd.get_dummies(ipo_processing,  columns=[columnWeTransform+'_year',columnWeTransform+'_month',columnWeTransform+'_day',columnWeTransform+'_weekday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date_amd : DONE\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from excel date to datetime\n",
    "ipo_processing['date_amd'] = replace_nan(ipo['date_amd'], '')\n",
    "ipo_processing['date_amd'] = ipo_processing['date_amd'].apply(lambda x: from_excel_ordinal(x) )\n",
    "ipo_processing['date_amd'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate difference between date_amd and offer_date save number of days as int\n",
    "ipo_processing['offer_date'] = ipo['offer_date'].apply(lambda x: pd.to_datetime(x) )\n",
    "ipo_processing['time_amd'] = ipo_processing.apply(lambda x: x['offer_date']-x['date_amd'], axis=1 ) #-x['offer_date'])\n",
    "num_col.append('time_amd')\n",
    "ipo_processing['time_amd'] = ipo_processing['time_amd'].apply(lambda x: x.days)\n",
    "\n",
    "#column to drop:\n",
    "columns_to_drop.append('date_amd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offering_Price: the price at which a company sells its shares to investors.\n",
    "\n",
    "Num_Shares: the total number of outstanding shares.\n",
    "\n",
    "Closing_Price: (at the end of the first day of training) price at which shares trade in the open market, measured at the end of the first day of trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 Pre-processing of Numerical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by numerical features that needs more preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum over all values in round_tot\n",
    "ipo_processing.round_tot = ipo_processing.round_tot.fillna(0)\n",
    "ipo_processing.round_tot = ipo_processing.round_tot.replace(',','',regex = True)\n",
    "ipo_processing.round_tot = ipo_processing.round_tot.apply(lambda x: x.split('\\n') if str(x).isdigit()==False else [x] )\n",
    "\n",
    "ipo_processing.round_tot = [sum([float(x) for x in j if re.match(\"^\\d+?\\.\\d+?$\", str(x)) or str(x).isdigit()])   for j in ipo_processing.round_tot.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing['mgt_fee'] =ipo_processing.mgt_fee.replace('Comb.', np.nan)\n",
    "ipo_processing['gross_spread'] = ipo_processing.gross_spread.replace('na', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace ' in some numerical features:\n",
    "col_to_replace = ['min_round_vexp','avg_round_vexp', 'max_round_vexp', 'min_firm_amt_vexp', 'avg_firm_amt_vexp', 'max_firm_amt_vexp',\\\n",
    "          'min_fund_amt_vexp', 'max_fund_amt_vexp']\n",
    "for col in col_to_replace:\n",
    "    ipo_processing[col] = ipo_processing[col].str.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill null values \n",
    "for col in num_col:\n",
    "    ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats = ipo.select_dtypes(include='float64')\n",
    "\n",
    "for col in floats.columns:\n",
    "    if not(col in columns_to_drop):\n",
    "        print(col)\n",
    "        ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ints.columns:\n",
    "     ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7 Pre-processing of Textual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk_Factors :  Textual description of all the risk / done \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *\n",
    "\n",
    "\n",
    "risks,risks_words = process_text_columns(ipo['Risk_Factors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF --------------------------------------------------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 500)\n",
    "risks_tfidf = vectorizer.fit_transform(risks)\n",
    "risks_tfidf = risks_tfidf.toarray()\n",
    "\n",
    "#Convert to a dataframe\n",
    "risks_tfidf = pd.DataFrame(risks_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnToDrop.append('Risk_Factors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check for the Nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columnToDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.drop(columns,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Visualize pattern of missing data\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(ipo_processing.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.columns[ipo_processing.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.9 : TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing['Target'] = ipo_processing.Closing_Price >= ipo_processing.Offering_Price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing.Closing_Price == ipo_processing.Offering_Price )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.Target = ipo_processing.Target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ipo_processing.Target.value_counts()\n",
    "\n",
    "sns.set()\n",
    "\n",
    "ones = counts[1]/len(counts) * 100\n",
    "zeros = counts[0]/len(counts) * 100\n",
    "labels = 'Price increased', 'Price decreased'\n",
    "plt.pie( [ones, zeros], labels=labels, autopct='%1.2f%%', startangle=360)\n",
    "plt.title('Distribution of target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour le preprocessing Je pense qu'il manque: Num_Shares, lockup_days, lockup_date Mais Num_shares est-ce qu'on l'utilise pour le target et donc pas pour les features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7 Features Reduction\n",
    "\n",
    "### 1. Check the correlation with the target\n",
    "    -> Drop bottom 5%\n",
    "### 2. Check the correlation between feature\n",
    "    -> Drop \n",
    "    \n",
    "### 0.7.1. Check the correlation with the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check type of values\n",
    "ipo_processing.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Devide dataframe in function of type.\n",
    "ints = ipo.select_dtypes(include='int64')\n",
    "floats = ipo.select_dtypes(include='float64')\n",
    "objects = ipo.select_dtypes(include='object')\n",
    "timestamps = ipo.select_dtypes(include='M8[ns]')\"\"\"\n",
    "\n",
    "print(ipo_processing.select_dtypes(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_dic = {}\n",
    "for column in ipo_processing.columns:\n",
    "    corr_dic[column] = abs(ipo_processing[column]\n",
    "                           .corr(ipo_processing['Target']))\n",
    "\n",
    "for w in sorted(corr_dic, key=corr_dic.get, reverse=True):\n",
    "    print (w, corr_dic[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.7.2. Check the correlation between feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(dataframe.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _________________________________________________________ \n",
    "## Part 1\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using non-text fields. By non-text fields, we mean all fields except the 'Risk_Factors'. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(non-text-fields) -> Probability of being in class 1 \n",
    "\n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit peut être mettre tous les modèles dans un autre notebook si on va les utiliser dans les autres parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## J'enlève les dates en attendant\n",
    "ipo_processing.drop( columns = ['lockup_date','lockup_days','Closing_Price','Offering_Price','first_trade_date','offer_date','issue_date'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ipo_processing['Target'] \n",
    "features = ipo_processing.drop(['Risk_Factors','Target'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate target and features into test and training and validation sets\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = seed)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "bbaseline_clf = DummyClassifier(strategy='stratified', random_state = seed)\n",
    "\n",
    "# Fit the dummy classifier \n",
    "bbaseline_clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict target probabilities of belonging to positive class\n",
    "y_pred = bbaseline_clf.predict_proba(X_test)\n",
    "\n",
    "# Compute area under the curve score\n",
    "print('auc',roc_auc_score(y_test, y_pred[:,1]))\n",
    "\n",
    "plot_roc_curve('Dummy',y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_clf = LogisticRegression() \n",
    "baseline_clf.fit(X_train,y_train)\n",
    "y_pred = baseline_clf.predict_proba(X_test)\n",
    "print('Score ',roc_auc_score(y_test, y_pred[:,1]))\n",
    "plot_roc_curve('Logistic Regression',y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression tuning c penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standardize features and classifier in a single pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lr_clf', LogisticRegression()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(lr_clf__penalty='l1')\n",
    "\n",
    "# Finding best value of C using validation set\n",
    "scores = []\n",
    "Cs = []\n",
    "for C in np.logspace(-4, 5, 10):\n",
    "    pipeline.set_params(lr_clf__C=C) \n",
    "    pipeline.fit(X_train,y_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_test)\n",
    "    scores.append(roc_auc_score(y_test, y_train_pred[:,1]))\n",
    "    Cs.append(C)\n",
    "\n",
    "best_C = Cs[scores.index(max(scores))]\n",
    "print ('best C = %d with auc score = %2.4f' %(best_C, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(lr_clf__C=best_C)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_lr = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_lr[:,1])\n",
    "print ('lr classifer auc with l1 regularization = %2.4f' %score)\n",
    "plot_roc_curve('l1 regularization',y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN tuning N neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features and classifier in a single pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('knn_clf', KNeighborsClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Finding best value of K using validation set\n",
    "scores = []\n",
    "Ks = []\n",
    "for K in [int(i) for i in np.linspace(5, 95, 10)]:\n",
    "    pipeline.set_params(knn_clf__n_neighbors = K) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    Ks.append(K)\n",
    "\n",
    "best_K = Ks[scores.index(max(scores))]\n",
    "print ('best K = %d with auc score = %2.4f' %(best_K, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(knn_clf__n_neighbors = best_K)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_knn = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_knn[:,1])\n",
    "print ('knn classifer auc = %2.4f' %score)\n",
    "plot_roc_curve('KNN',y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define a random classifier pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(rf_clf__random_state = seed)\n",
    "    \n",
    "# Finding best value of n_estimators using validation set\n",
    "scores = []\n",
    "NSs = []\n",
    "for NS in [int(i) for i in np.linspace(10, 100, 10)]:\n",
    "    pipeline.set_params(rf_clf__n_estimators = NS) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    NSs.append(NS)\n",
    "\n",
    "best_NS = NSs[scores.index(max(scores))]\n",
    "print ('best NS = %d with auc score = %2.4f' %(best_NS, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(rf_clf__n_estimators = best_NS)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_rf = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_rf[:,1])\n",
    "print ('rf classifer auc = %2.4f' %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve('KNN',y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define a random classifier pipeline\n",
    "estimators = []\n",
    "estimators.append(('gb_clf', GradientBoostingClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(gb_clf__random_state = seed)\n",
    "    \n",
    "# Finding best value of n_estimators using validation set\n",
    "scores = []\n",
    "NSs = []\n",
    "for NS in [int(i) for i in np.linspace(10, 100, 10)]:\n",
    "    pipeline.set_params(gb_clf__n_estimators = NS) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    NSs.append(NS)\n",
    "\n",
    "best_NS = NSs[scores.index(max(scores))]\n",
    "print ('best NS = %d with auc score = %2.4f' %(best_NS, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(gb_clf__n_estimators = best_NS)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_gb = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_gb[:,1])\n",
    "plot_roc_curve('GradientBoostingClassifier',y_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using __only__ textual field 'Risk_Factors'. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(text-fields) -> Probability of being in class 1 \n",
    "\n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "def nested_cv(X, y, est_pipe, p_grid, p_score, n_splits_inner = 3, n_splits_outer = 3, n_cores = 1, seed = 0):\n",
    "\n",
    "    # Cross-validation schema for inner and outer loops (stratified if it is a classification)\n",
    "    inner_cv = KFold(n_splits = n_splits_inner, shuffle = True, random_state = seed)\n",
    "    outer_cv = KFold(n_splits = n_splits_outer, shuffle = True, random_state = seed)\n",
    "    \n",
    "    # Grid search to tune hyper parameters\n",
    "    est = GridSearchCV(estimator = est_pipe, param_grid = p_grid, cv = inner_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_scores = cross_val_score(estimator = est, X = X, y = y, cv = outer_cv, scoring = p_score, n_jobs = n_cores)\n",
    "    \n",
    "    print('Average score: %0.4f (+/- %0.4f)' % (nested_scores.mean(), nested_scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of Risk Factors :  risks_tfidf    \n",
    "features = risks_tfidf\n",
    "seed = 0\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regroupe to 50 Fields \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "principalComponents = pca.fit_transform(risks_tfidf)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "\n",
    "\n",
    "\n",
    "#Update the name of the columns\n",
    "# get length of df's columns\n",
    "num_cols = 100\n",
    "# generate range of ints for suffixes\n",
    "rng = range(0,num_cols)\n",
    "\n",
    "new_cols = [ 'risk_'+str(i) for i in rng]\n",
    "\n",
    "# ensure the length of the new columns list is equal to the length of df's columns\n",
    "principalComponents.columns = new_cols[:num_cols]\n",
    "\n",
    "principalComponents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = principalComponents\n",
    "seed = 0\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in [10, 20, 50, 100]]}\n",
    "\n",
    "nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word Vectors : Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def review_to_wordlist( review ):\n",
    "    \n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "   \n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences, where each sentence is a word list\n",
    "\n",
    "def review_to_sentences( review, tokenizer ):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())  \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:      \n",
    "        if len(raw_sentence) > 0:           \n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "   \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = []\n",
    "risks = ipo['Risk_Factors']\n",
    "for risk in risks:\n",
    "    sentences += review_to_sentences(risk, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word vectors\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # word vector dimensionality                      \n",
    "min_word_count = 40   # minimum word count                        \n",
    "num_workers = 16      # number of threads to run in parallel\n",
    "context = 10          # context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model \n",
    "from gensim.models import word2vec\n",
    "print ('Training model...')\n",
    "w2v_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "print ('Done !')\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient\n",
    "w2v_model.init_sims(replace=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all of the word vectors in a given paragraph\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0.\n",
    "     \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Given a set of reviews (each one a list of words), calculate \n",
    "# the average feature vector for each one and return a 2D numpy array\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype='float32')\n",
    "     \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000 == 0:\n",
    "           print ('Review %d of %d' % (counter, len(reviews)))\n",
    "       \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature vectors for review data,\n",
    "# using the functions we defined above.\n",
    "\n",
    "clean_data_reviews = []\n",
    "for review in ipo['Risk_Factors']:\n",
    "    clean_data_reviews.append( review_to_wordlist( review ))\n",
    "\n",
    "w2v_features = getAvgFeatureVecs( clean_data_reviews, w2v_model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Random Forest with Average Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph vectors : Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec needs each review to be tagged with some sort of ids\n",
    "# Here we tag each review with the 'id' field\n",
    "\n",
    "import gensim\n",
    "\n",
    "tagged_clean_data_reviews = []\n",
    "for uid, review in zip(data['id'], clean_data_reviews):\n",
    "    tagged_clean_data_reviews.append(gensim.models.doc2vec.TaggedDocument(words=review, tags=['%s' % uid[1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using __all__ fields. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(all-fields) -> Probability of being in class 1 \n",
    "    \n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "  \n",
    "Predict the share price at the end of the day using __all__ fields.\n",
    "\n",
    "    f(all-fields) ->  Share price at the end of the first day of trading\n",
    "    \n",
    "For the evaluation metric, report statisitcs for R-squared, Residual Mean Squared Error, Mean Absolute Error, and Median Absolute Error; however, be sure to tune and hypertune your models using R-Squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "As mentioned earlier, you can find the dataset under project directory in the course git repository under the name of *ipo.xlsx*. The description of each variable can also be found in *variable_description.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "We expect your solution for each step to contain the following:\n",
    "\n",
    "* data preprocessing and feature extraction (can be shared across different steps)\n",
    "* feature reduction\n",
    "* train, tune and test different predictive models\n",
    "* model comparison and arguing about the best model (don't forget mentioning a baseline model)\n",
    "* predict the labels of the to_predict dataset using your final model\n",
    "* discussion on possible additional tasks that can be done to boost the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "* Predict values from your *best* predictive model for the target variable in Parts 1 to 4 above, and insert those values into the file __*ipo_to_predict.xlsx*__ The fields to be completed by you are: __Price_Change_Non_Textual__ (Part 1), __Price_Change_Textual__ (Part 2), __Price_Change_All__ (Part 3), and __Price_All__ (Part 4).  \n",
    "  \n",
    "  \n",
    "* Deliver a Jupyter notebook with an explanation of your methods, codes and results. Don't forget to divide your notebook into different parts, which clearly shows your solution to the common pre-processing as well as different steps separately. \n",
    "    \n",
    "    \n",
    "\n",
    "* Submit your final notebook and files into the git repository of the team (we will create that git repo for you).\n",
    "\n",
    "\n",
    "* Present your results in the final session of the course. Communicate them in a clear and concise manner. The goal is to learn how to present your results to stakeholders at the right level of detail. **We will discuss this more in classe**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "* Take some time at the start of the project to educate yourslef about the IPO process. We privde you wiht two main texts in the class repository under *resources* folder. Understanding how variables relate to the target outcomes will help you to construct new measures from the tabularized data and/or selecting or eliminating features that relate to the target variable.  \n",
    "\n",
    "\n",
    "* Present your results as a story - this is very important!   \n",
    "  \n",
    "\n",
    "* Document all of your assumptions (e.g. evaluation metric, hyper-parameter values, ...).  \n",
    "\n",
    "\n",
    "* Make sure your code will run and results are reproducible (fix random seeds, etc.).  \n",
    "\n",
    "\n",
    "*  Comment your blocks of code (and lines of code if needed) and anything in your story/logic that might not be obvious by looking at your code.    \n",
    "\n",
    "\n",
    "* To speed up experimentation, you might use a small sample of the original dataset to do your initial coding. Also try to use all possible cores for computation, by setting the option of n_jobs = -1, when needed. \n",
    "\n",
    "\n",
    "* Try to be creative to improve your predictions, but don't forget that it is also important to explain your line of thinking/reasoning.\n",
    "\n",
    "\n",
    "* Your final grade is based on the whole process of doing the project and not just based on your results on the unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Grading of the project (apart from presentation), is based on the following components:\n",
    "    \n",
    "    20 %  Documentation and organization of your notebook\n",
    "    15 %  Quality and commenting of code\n",
    "    10 %  Pre-processing\n",
    "    15 %  Part 1\n",
    "    15 %  Part 2\n",
    "    10 %  Part 3\n",
    "    15 %  Part 4\n",
    "    \n",
    "     5%   Bonus Contest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional bonus contest at the end of the project, we will award an extra 5% of the total project grade to the team that comes up with the \"best\" strategy for investing into IPOs based on your estimated model(s). Specifically, assume you have USD 1,000,000 to invest into the IPO stocks that appear in the \"unseen\" file __*ipo_to_predict.xlsx*__. In the column \"Your_Bet\", allocate some portion of that USD 1,000,000 to each of the stocks listed in the unseen file. The total allocation must sum to $1,000,000. The top team making the most money (once outcomes are revealed at time of grading) will earn the 5% bonus. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
