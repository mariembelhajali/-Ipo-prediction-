{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFB Course Project: Predicting IPO Share Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg)\n",
    "\n",
    "image source : https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Initial Public Offering (IPO) is the process by which a private company becomes publicly traded on a stock exchange. The IPO company offers its shares to public investors in exchange of capital for sustaining expansion and growth. For this reason, IPOs are often issued by small or young companies, but they can also be done by large  companies looking to become publicly traded. During an IPO, the company obtains the assistance of an investment bank (underwriter), which helps determine the type, amount and price of the shares being offered. Decisions about the offering price are particularly important to avoid incurring excessive costs and maximize the capital received in the IPO. However at the end of the first trading day, price of each share can change due to market dynamics, which can lead to a price higher or lower than the offering one.\n",
    "\n",
    "During an Initial Public Offering (IPO), the firm’s management have to disclose all relevant information about their business in a filing with the government called the \"IPO Prospectus.\" Although there might be concerns about the public disclosure of sensitive information in the Prospectus that can help competitors, firms are encouraged to be as transparent as possible in order to avoid future litigation (lawsuits). A key textual field from the prospectus is:\n",
    "\n",
    "__Risk_Factors__: Firms have to disclose all relevant information about internal or external risk factors that might affect future business performances. This information is contained in the “Risk Factors” section of the IPO prospectus. \n",
    "\n",
    "The key pricing variables are:\n",
    "\n",
    "__Offering_Price__: the price at which a company sells its shares to investors.\n",
    "\n",
    "__Num_Shares__: the total number of outstanding shares.\n",
    "\n",
    "__Closing_Price__: (at the end of the first day of training) price at which shares trade in the open market, measured at the end of the first day of trading.\n",
    "\n",
    "In this project you are provided with IPO data of different firms that are collected from different sources. You can find the dataset under project directory in the course git repository under the name of *ipo.xlsx*. The description of other variables can be found in *variable_description.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will be presented as follow :\n",
    "\n",
    "# Part 1 :\n",
    "# Part 2 :\n",
    "# Part 3 :\n",
    "# Part 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5547c5031a8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel_helpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd     \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re as re\n",
    "import nltk\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from model_helpers import *\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot_functions import *\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import Dataset \n",
    "\n",
    "##### Import,  explore dataset and check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "DATA_FOLDER = 'data'\n",
    "\n",
    "#Cost TIME\n",
    "ipo = pd.read_csv(DATA_FOLDER+'/data_non_textual_clean.csv')\n",
    "ipo_predict = pd.read_csv(DATA_FOLDER+'/data_non_textual_clean_predict.csv')\n",
    "\n",
    "ipo_text =  pd.read_csv(DATA_FOLDER+'/data_textual_clean.csv')\n",
    "ipo_text_predict = pd.read_csv(DATA_FOLDER+'/data_textual_clean_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3721)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo.columns[ipo.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474, 3721)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 501)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Closing_Price</th>\n",
       "      <th>Num_Shares</th>\n",
       "      <th>Offering_Price</th>\n",
       "      <th>SEC_fee</th>\n",
       "      <th>SP1</th>\n",
       "      <th>SP3</th>\n",
       "      <th>acc_fee</th>\n",
       "      <th>amd_hp</th>\n",
       "      <th>amd_pr_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>56</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>71</th>\n",
       "      <th>81</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>18.303713</td>\n",
       "      <td>8125000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33761.715</td>\n",
       "      <td>1367.69</td>\n",
       "      <td>1845.16</td>\n",
       "      <td>642375.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3001</td>\n",
       "      <td>18.303713</td>\n",
       "      <td>14229999</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5925.000</td>\n",
       "      <td>1410.69</td>\n",
       "      <td>1981.57</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3002</td>\n",
       "      <td>18.303713</td>\n",
       "      <td>94898000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>47843.000</td>\n",
       "      <td>1372.82</td>\n",
       "      <td>1867.63</td>\n",
       "      <td>3500000.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3003</td>\n",
       "      <td>18.303713</td>\n",
       "      <td>13426000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4939.000</td>\n",
       "      <td>1519.48</td>\n",
       "      <td>2124.29</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3004</td>\n",
       "      <td>18.303713</td>\n",
       "      <td>79990000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38085.000</td>\n",
       "      <td>1530.33</td>\n",
       "      <td>2105.20</td>\n",
       "      <td>1050000.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3721 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Closing_Price  Num_Shares  Offering_Price    SEC_fee      SP1  \\\n",
       "0        3000      18.303713     8125000            12.0  33761.715  1367.69   \n",
       "1        3001      18.303713    14229999            11.0   5925.000  1410.69   \n",
       "2        3002      18.303713    94898000            16.0  47843.000  1372.82   \n",
       "3        3003      18.303713    13426000             5.0   4939.000  1519.48   \n",
       "4        3004      18.303713    79990000            14.0  38085.000  1530.33   \n",
       "\n",
       "       SP3    acc_fee  amd_hp  amd_pr_amt   ...    51  52  53  54  56  61  62  \\\n",
       "0  1845.16   642375.7    14.0        30.0   ...     0   0   0   0   0   0   0   \n",
       "1  1981.57   160000.0    13.0        40.0   ...     0   0   0   0   0   0   0   \n",
       "2  1867.63  3500000.0    17.0       304.0   ...     0   0   0   0   0   1   0   \n",
       "3  2124.29    75000.0     5.0        33.0   ...     0   0   0   0   0   0   0   \n",
       "4  2105.20  1050000.0    19.0       270.0   ...     0   0   0   0   0   0   0   \n",
       "\n",
       "   71  81  Target  \n",
       "0   0   0       1  \n",
       "1   0   0       1  \n",
       "2   0   0       1  \n",
       "3   0   0       1  \n",
       "4   0   0       1  \n",
       "\n",
       "[5 rows x 3721 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo_predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _________________________________________________________ \n",
    "## Part 1\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using non-text fields. By non-text fields, we mean all fields except the 'Risk_Factors'. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(non-text-fields) -> Probability of being in class 1 \n",
    "\n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit peut être mettre tous les modèles dans un autre notebook si on va les utiliser dans les autres parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## J'enlève les dates en attendant\n",
    "ipo_processing.drop( columns = ['lockup_date','lockup_days','Closing_Price','Offering_Price','first_trade_date','offer_date','issue_date'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ipo['Target'] \n",
    "features = ipo.drop(['Risk_Factors','Target'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate target and features into test and training and validation sets\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = seed)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "bbaseline_clf = DummyClassifier(strategy='stratified', random_state = seed)\n",
    "\n",
    "# Fit the dummy classifier \n",
    "bbaseline_clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict target probabilities of belonging to positive class\n",
    "y_pred = bbaseline_clf.predict_proba(X_test)\n",
    "\n",
    "# Compute area under the curve score\n",
    "baseline = roc_auc_score(y_test, y_pred[:,1])\n",
    "print('auc',baseline)\n",
    "\n",
    "plot_roc_curve('Dummy',y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_clf = LogisticRegression() \n",
    "baseline_clf.fit(X_train,y_train)\n",
    "y_pred = baseline_clf.predict_proba(X_test)\n",
    "print('Score ',roc_auc_score(y_test, y_pred[:,1]))\n",
    "plot_roc_curve('Logistic Regression',y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression tuning c penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standardize features and classifier in a single pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lr_clf', LogisticRegression()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(lr_clf__penalty='l1')\n",
    "\n",
    "# Finding best value of C using validation set\n",
    "scores = []\n",
    "Cs = []\n",
    "for C in np.logspace(-4, 5, 10):\n",
    "    pipeline.set_params(lr_clf__C=C) \n",
    "    pipeline.fit(X_train,y_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_test)\n",
    "    scores.append(roc_auc_score(y_test, y_train_pred[:,1]))\n",
    "    Cs.append(C)\n",
    "\n",
    "best_C = Cs[scores.index(max(scores))]\n",
    "print ('best C = %d with auc score = %2.4f' %(best_C, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(lr_clf__C=best_C)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_lr = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_lr[:,1])\n",
    "print ('lr classifer auc with l1 regularization = %2.4f' %score)\n",
    "plot_roc_curve('l1 regularization',y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN tuning N neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features and classifier in a single pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('knn_clf', KNeighborsClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Finding best value of K using validation set\n",
    "scores = []\n",
    "Ks = []\n",
    "for K in [int(i) for i in np.linspace(5, 95, 10)]:\n",
    "    pipeline.set_params(knn_clf__n_neighbors = K) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    Ks.append(K)\n",
    "\n",
    "best_K = Ks[scores.index(max(scores))]\n",
    "print ('best K = %d with auc score = %2.4f' %(best_K, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(knn_clf__n_neighbors = best_K)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_knn = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_knn[:,1])\n",
    "print ('knn classifer auc = %2.4f' %score)\n",
    "plot_roc_curve('KNN',y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define a random classifier pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(rf_clf__random_state = seed)\n",
    "    \n",
    "# Finding best value of n_estimators using validation set\n",
    "scores = []\n",
    "NSs = []\n",
    "for NS in [int(i) for i in np.linspace(10, 100, 10)]:\n",
    "    pipeline.set_params(rf_clf__n_estimators = NS) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    NSs.append(NS)\n",
    "\n",
    "best_NS = NSs[scores.index(max(scores))]\n",
    "print ('best NS = %d with auc score = %2.4f' %(best_NS, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(rf_clf__n_estimators = best_NS)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_rf = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_rf[:,1])\n",
    "print ('rf classifer auc = %2.4f' %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve('KNN',y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define a random classifier pipeline\n",
    "estimators = []\n",
    "estimators.append(('gb_clf', GradientBoostingClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(gb_clf__random_state = seed)\n",
    "    \n",
    "# Finding best value of n_estimators using validation set\n",
    "scores = []\n",
    "NSs = []\n",
    "for NS in [int(i) for i in np.linspace(10, 100, 10)]:\n",
    "    pipeline.set_params(gb_clf__n_estimators = NS) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    NSs.append(NS)\n",
    "\n",
    "best_NS = NSs[scores.index(max(scores))]\n",
    "print ('best NS = %d with auc score = %2.4f' %(best_NS, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(gb_clf__n_estimators = best_NS)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_gb = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_gb[:,1])\n",
    "plot_roc_curve('GradientBoostingClassifier',y_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using __only__ textual field 'Risk_Factors'. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(text-fields) -> Probability of being in class 1 \n",
    "\n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1e869df901e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Copy dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipo_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "#Copy dataframe\n",
    "text_features = ipo_text.copy()\n",
    "text_target = target[0:len(text_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "def nested_cv(X, y, est_pipe, p_grid, p_score, n_splits_inner = 3, n_splits_outer = 3, n_cores = 1, seed = 0):\n",
    "\n",
    "    # Cross-validation schema for inner and outer loops (stratified if it is a classification)\n",
    "    inner_cv = KFold(n_splits = n_splits_inner, shuffle = True, random_state = seed)\n",
    "    outer_cv = KFold(n_splits = n_splits_outer, shuffle = True, random_state = seed)\n",
    "    \n",
    "    # Grid search to tune hyper parameters\n",
    "    est = GridSearchCV(estimator = est_pipe, param_grid = p_grid, cv = inner_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_scores = cross_val_score(estimator = est, X = X, y = y, cv = outer_cv, scoring = p_score, n_jobs = n_cores)\n",
    "    \n",
    "    print('Average score: %0.4f (+/- %0.4f)' % (nested_scores.mean(), nested_scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of Risk Factors :  risks_tfidf    \n",
    "features = risks_tfidf\n",
    "seed = 0\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regroupe to 50 Fields \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "principalComponents = pca.fit_transform(risks_tfidf)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "\n",
    "\n",
    "\n",
    "#Update the name of the columns\n",
    "# get length of df's columns\n",
    "num_cols = 100\n",
    "# generate range of ints for suffixes\n",
    "rng = range(0,num_cols)\n",
    "\n",
    "new_cols = [ 'risk_'+str(i) for i in rng]\n",
    "\n",
    "# ensure the length of the new columns list is equal to the length of df's columns\n",
    "principalComponents.columns = new_cols[:num_cols]\n",
    "\n",
    "principalComponents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = principalComponents\n",
    "seed = 0\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in [10, 20, 50, 100]]}\n",
    "\n",
    "nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word Vectors : Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def review_to_wordlist( review ):\n",
    "    \n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "   \n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences, where each sentence is a word list\n",
    "\n",
    "def review_to_sentences( review, tokenizer ):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())  \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:      \n",
    "        if len(raw_sentence) > 0:           \n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "   \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = []\n",
    "risks = ipo['Risk_Factors']\n",
    "for risk in risks:\n",
    "    sentences += review_to_sentences(risk, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word vectors\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # word vector dimensionality                      \n",
    "min_word_count = 40   # minimum word count                        \n",
    "num_workers = 16      # number of threads to run in parallel\n",
    "context = 10          # context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model \n",
    "from gensim.models import word2vec\n",
    "print ('Training model...')\n",
    "w2v_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "print ('Done !')\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient\n",
    "w2v_model.init_sims(replace=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all of the word vectors in a given paragraph\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0.\n",
    "     \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Given a set of reviews (each one a list of words), calculate \n",
    "# the average feature vector for each one and return a 2D numpy array\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype='float32')\n",
    "     \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000 == 0:\n",
    "           print ('Review %d of %d' % (counter, len(reviews)))\n",
    "       \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature vectors for review data,\n",
    "# using the functions we defined above.\n",
    "\n",
    "clean_data_reviews = []\n",
    "for review in ipo['Risk_Factors']:\n",
    "    clean_data_reviews.append( review_to_wordlist( review ))\n",
    "\n",
    "w2v_features = getAvgFeatureVecs( clean_data_reviews, w2v_model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Random Forest with Average Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph vectors : Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec needs each review to be tagged with some sort of ids\n",
    "# Here we tag each review with the 'id' field\n",
    "\n",
    "import gensim\n",
    "\n",
    "tagged_clean_data_reviews = []\n",
    "for uid, review in zip(data['id'], clean_data_reviews):\n",
    "    tagged_clean_data_reviews.append(gensim.models.doc2vec.TaggedDocument(words=review, tags=['%s' % uid[1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using __all__ fields. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(all-fields) -> Probability of being in class 1 \n",
    "    \n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all features and get train, validate and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat([text_features, features], axis=1)\n",
    "\n",
    "# Separate target and features into test and training and validation sets\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, target, test_size = 0.2, random_state = seed)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names must be unique",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c01fdeed4d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mNS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mxgb_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_clf__n_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mxgb_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 541\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.n_estimators,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mfeature_names\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_names must be unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'feature_names must have the same length as data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names must be unique"
     ]
    }
   ],
   "source": [
    "# Define pipeline\n",
    "estimators = []\n",
    "estimators.append(('xgb_clf', XGBClassifier()))\n",
    "xgb_pipe = Pipeline(estimators)\n",
    "xgb_pipe.set_params(xgb_clf__n_jobs = -1)\n",
    "xgb_pipe.set_params(xgb_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'roc_auc'\n",
    "\n",
    "# Finding best value of n_estimators using validation set\n",
    "scores = []\n",
    "NSs = []\n",
    "for NS in [int(i) for i in np.linspace(10, 100, 10)]:\n",
    "    xgb_pipe.set_params(xgb_clf__n_estimators = NS) \n",
    "    xgb_pipe.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = xgb_pipe.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    NSs.append(NS)\n",
    "\n",
    "best_NS = NSs[scores.index(max(scores))]\n",
    "print ('best NS = %d with auc score = %2.4f' %(best_NS, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "xgb_pipe.set_params(xgb_clf__n_estimators = best_NS)\n",
    "xgb_pipe.fit(X_train,y_train)\n",
    "y_pred_gb = xgb_pipe.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_gb[:,1])\n",
    "plot_roc_curve('GradientBoostingClassifier',y_pred_gb,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2b1abe13880e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Train and evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#Train the pipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mnn_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_train\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Get predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mnn1_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \"\"\"\n\u001b[1;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m--> 612\u001b[0;31m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Fix random seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Build neural network model\n",
    "def create_model(nbr_l1, nbr_l2, nbr_l3):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nbr_l1, input_dim=all_features.shape[1], activation='relu'))\n",
    "    model.add(Dense(nbr_l2, activation='relu'))\n",
    "    model.add(Dense(nbr_l3, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nn_score = []\n",
    "for i in range(3,7,1):\n",
    "    nbr_l1 = pow(2,(i+2))\n",
    "    nbr_l2 = pow(2,(i+1))\n",
    "    nbr_l3 = pow(2,(i))\n",
    "    model = create_model(nbr_l1, nbr_l2, nbr_l3)\n",
    "    \n",
    "    # Create pipeline using keras wrapper for sklearn\n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append(('nn_clf', KerasClassifier(build_fn=create_model)))\n",
    "    nn_pipe = Pipeline(estimators)\n",
    "    nn_pipe.set_params(nn_clf__epochs = 2)\n",
    "    nn_pipe.set_params(nn_clf__batch_size = 10)\n",
    "    nn_pipe.set_params(nn_clf__verbose = 2)\n",
    "\n",
    "    # Setup possible values of parameters to optimize over\n",
    "    p_grid = {}\n",
    "\n",
    "    # Fixed parameters\n",
    "    score = 'roc_auc'\n",
    "\n",
    "    # Train and evaluate model\n",
    "    #Train the pipe\n",
    "    nn_pipe.fit(X_train_train, y_train_train )\n",
    "    # Get predicted values\n",
    "    nn1_predict = nn_pipe.predict_proba(X_train_val)\n",
    "    score = roc_auc_score(y_train_val, nn_predict[:,1])\n",
    "    print(\"\")\n",
    "    nn_score.append(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "  \n",
    "Predict the share price at the end of the day using __all__ fields.\n",
    "\n",
    "    f(all-fields) ->  Share price at the end of the first day of trading\n",
    "    \n",
    "For the evaluation metric, report statisitcs for R-squared, Residual Mean Squared Error, Mean Absolute Error, and Median Absolute Error; however, be sure to tune and hypertune your models using R-Squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "As mentioned earlier, you can find the dataset under project directory in the course git repository under the name of *ipo.xlsx*. The description of each variable can also be found in *variable_description.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "We expect your solution for each step to contain the following:\n",
    "\n",
    "* data preprocessing and feature extraction (can be shared across different steps)\n",
    "* feature reduction\n",
    "* train, tune and test different predictive models\n",
    "* model comparison and arguing about the best model (don't forget mentioning a baseline model)\n",
    "* predict the labels of the to_predict dataset using your final model\n",
    "* discussion on possible additional tasks that can be done to boost the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "* Predict values from your *best* predictive model for the target variable in Parts 1 to 4 above, and insert those values into the file __*ipo_to_predict.xlsx*__ The fields to be completed by you are: __Price_Change_Non_Textual__ (Part 1), __Price_Change_Textual__ (Part 2), __Price_Change_All__ (Part 3), and __Price_All__ (Part 4).  \n",
    "  \n",
    "  \n",
    "* Deliver a Jupyter notebook with an explanation of your methods, codes and results. Don't forget to divide your notebook into different parts, which clearly shows your solution to the common pre-processing as well as different steps separately. \n",
    "    \n",
    "    \n",
    "\n",
    "* Submit your final notebook and files into the git repository of the team (we will create that git repo for you).\n",
    "\n",
    "\n",
    "* Present your results in the final session of the course. Communicate them in a clear and concise manner. The goal is to learn how to present your results to stakeholders at the right level of detail. **We will discuss this more in classe**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "* Take some time at the start of the project to educate yourslef about the IPO process. We privde you wiht two main texts in the class repository under *resources* folder. Understanding how variables relate to the target outcomes will help you to construct new measures from the tabularized data and/or selecting or eliminating features that relate to the target variable.  \n",
    "\n",
    "\n",
    "* Present your results as a story - this is very important!   \n",
    "  \n",
    "\n",
    "* Document all of your assumptions (e.g. evaluation metric, hyper-parameter values, ...).  \n",
    "\n",
    "\n",
    "* Make sure your code will run and results are reproducible (fix random seeds, etc.).  \n",
    "\n",
    "\n",
    "*  Comment your blocks of code (and lines of code if needed) and anything in your story/logic that might not be obvious by looking at your code.    \n",
    "\n",
    "\n",
    "* To speed up experimentation, you might use a small sample of the original dataset to do your initial coding. Also try to use all possible cores for computation, by setting the option of n_jobs = -1, when needed. \n",
    "\n",
    "\n",
    "* Try to be creative to improve your predictions, but don't forget that it is also important to explain your line of thinking/reasoning.\n",
    "\n",
    "\n",
    "* Your final grade is based on the whole process of doing the project and not just based on your results on the unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Grading of the project (apart from presentation), is based on the following components:\n",
    "    \n",
    "    20 %  Documentation and organization of your notebook\n",
    "    15 %  Quality and commenting of code\n",
    "    10 %  Pre-processing\n",
    "    15 %  Part 1\n",
    "    15 %  Part 2\n",
    "    10 %  Part 3\n",
    "    15 %  Part 4\n",
    "    \n",
    "     5%   Bonus Contest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional bonus contest at the end of the project, we will award an extra 5% of the total project grade to the team that comes up with the \"best\" strategy for investing into IPOs based on your estimated model(s). Specifically, assume you have USD 1,000,000 to invest into the IPO stocks that appear in the \"unseen\" file __*ipo_to_predict.xlsx*__. In the column \"Your_Bet\", allocate some portion of that USD 1,000,000 to each of the stocks listed in the unseen file. The total allocation must sum to $1,000,000. The top team making the most money (once outcomes are revealed at time of grading) will earn the 5% bonus. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
