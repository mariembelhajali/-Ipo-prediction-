{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFB Course Project: Predicting IPO Share Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg)\n",
    "\n",
    "image source : https://img.etimg.com/thumb/height-480,width-640,msid-64038320,imgsize-108012/stock-market2-getty-images.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Initial Public Offering (IPO) is the process by which a private company becomes publicly traded on a stock exchange. The IPO company offers its shares to public investors in exchange of capital for sustaining expansion and growth. For this reason, IPOs are often issued by small or young companies, but they can also be done by large  companies looking to become publicly traded. During an IPO, the company obtains the assistance of an investment bank (underwriter), which helps determine the type, amount and price of the shares being offered. Decisions about the offering price are particularly important to avoid incurring excessive costs and maximize the capital received in the IPO. However at the end of the first trading day, price of each share can change due to market dynamics, which can lead to a price higher or lower than the offering one.\n",
    "\n",
    "During an Initial Public Offering (IPO), the firm’s management have to disclose all relevant information about their business in a filing with the government called the \"IPO Prospectus.\" Although there might be concerns about the public disclosure of sensitive information in the Prospectus that can help competitors, firms are encouraged to be as transparent as possible in order to avoid future litigation (lawsuits). A key textual field from the prospectus is:\n",
    "\n",
    "__Risk_Factors__: Firms have to disclose all relevant information about internal or external risk factors that might affect future business performances. This information is contained in the “Risk Factors” section of the IPO prospectus. \n",
    "\n",
    "The key pricing variables are:\n",
    "\n",
    "__Offering_Price__: the price at which a company sells its shares to investors.\n",
    "\n",
    "__Num_Shares__: the total number of outstanding shares.\n",
    "\n",
    "__Closing_Price__: (at the end of the first day of training) price at which shares trade in the open market, measured at the end of the first day of trading.\n",
    "\n",
    "In this project you are provided with IPO data of different firms that are collected from different sources. You can find the dataset under project directory in the course git repository under the name of *ipo.xlsx*. The description of other variables can be found in *variable_description.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will be presented as follow :\n",
    "\n",
    "# Part 0 : Pre Processing\n",
    "#####  libraries & useful functions\n",
    "### 0.1 Import Dataset\n",
    "### 0.2 Pre-processsing of Special Features\n",
    "### 0.3 Pre-processing of Categorical Features\n",
    "### 0.4 Pre-processing of Ordinal Features\n",
    "### 0.5 Pre-processing of Dates Features\n",
    "### 0.6 Pre-processing of Numerical Features\n",
    "### 0.7 Pre-processing of Textual Features\n",
    "### 0.8 Target\n",
    "### 0.9 Feature Reduction\n",
    "        1 Correlation with target\n",
    "        2 Correlation between features\n",
    "# Part 1 :\n",
    "# Part 2 :\n",
    "# Part 3 :\n",
    "# Part 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 : Pre-Processing\n",
    "\n",
    "##### Libraries & Useful functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import re as re\n",
    "import nltk\n",
    "import datetime as dt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import roc_curve\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import Dataset \n",
    "\n",
    "##### Import,  explore dataset and check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "DATA_FOLDER = 'data'\n",
    "ipo = pd.read_excel(DATA_FOLDER + '/ipo.xlsx')\n",
    "ipo_to_predict = pd.read_excel(DATA_FOLDER+'/ipo_to_predict.xlsx')\n",
    "\n",
    "#merge\n",
    "ipo = ipo.append(ipo_to_predict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaration of categorization table to use in the data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = ['amd_nbr','round_tot','mgt_fee', 'gross_spread', 'min_round_vexp','avg_round_vexp', 'max_round_vexp',\\\n",
    "           'min_firm_amt_vexp', 'avg_firm_amt_vexp', 'max_firm_amt_vexp',\\\n",
    "          'min_fund_amt_vexp', 'max_fund_amt_vexp','description_numeric', 'rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['exch', 'mgrs_role', 'mgrs','all_sic','auditor', 'city','description','ht_ind', 'ht_ind_gr','industry', 'ind_group', 'issuer',\\\n",
    "          'veic_descr','legal', 'naic_primary','naic_decr', 'public_descr', 'lockup_flag','sic_main', 'nation', 'lbo', 'prim_naic', 'prim_uop', 'pe_backed', 'shs_out_after',\\\n",
    "          'state', 'uop', 'vc', 'zip','description_cat' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columns to Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "columns_to_drop.append('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Closing_Price</th>\n",
       "      <th>Offering_Price</th>\n",
       "      <th>Risk_Factors</th>\n",
       "      <th>mgt_fee</th>\n",
       "      <th>pctchg_dj_1</th>\n",
       "      <th>pctchg_hp</th>\n",
       "      <th>pctchg_lp</th>\n",
       "      <th>pctchg_mp</th>\n",
       "      <th>pctchg_nasdaq_1</th>\n",
       "      <th>...</th>\n",
       "      <th>num_funds_vexp</th>\n",
       "      <th>min_round_vexp</th>\n",
       "      <th>avg_round_vexp</th>\n",
       "      <th>max_round_vexp</th>\n",
       "      <th>min_firm_amt_vexp</th>\n",
       "      <th>avg_firm_amt_vexp</th>\n",
       "      <th>max_firm_amt_vexp</th>\n",
       "      <th>min_fund_amt_vexp</th>\n",
       "      <th>max_fund_amt_vexp</th>\n",
       "      <th>Num_Shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, Closing_Price, Offering_Price, Risk_Factors, mgt_fee, pctchg_dj_1, pctchg_hp, pctchg_lp, pctchg_mp, pctchg_nasdaq_1, pctchg_sp_1, pctchg_sp_2, pctchg_dj_2, pctchg_nasdaq_2, pctchg_p, pctchg_sp_3, pctchg_sp_4, pctchg_amd_amt, pct_ins_shs_aft, pct_ins_shs_bef, pct_int_shs, mkt_cap, acc_fee, exch, mgrs_role, mgrs, all_sic, amd_hp, amd_lp, amd_mp, amd_pr_shs_pct, amd_pr_amt, amd_pr_shst_tot, amd_shst_tot, amd_date, amt_filed, auditor, bluesky, book_proceeds, book_proceeds_ovt, bvps, bvps_bef_offer, br, city, comm_eq, comm_eq_bef, dj_avg_1, dj_avg_2, date_amd, date_found, days_in_registr, deal_size, description, round_tot, exp_pctofproceeds, exp_incl_gross, filing_date, first_trade_date, free_float, gross_spread, gross_spread_allmkt, ht_ind, ht_ind_gr, offer_date, issue_date, industry, ind_group, int_aft, int_bef, intern)shs, price_range, issuer, veic_descr, legal, naic_primary, naic_decr, public_descr, lastamd_tot, legal_exp, lockup_date, lockup_days, lockup_flag, sic_main, mktval_aft, mktval_bef, misc_exp, h_fil_p, l_fil_p, m_fil_p, nasdaq_avg_1, nasdaq_avg_2, nation, num_mgrs, num_amd, num_bokr, num_bokr_up, num_emp, num_emp_date, num_lead_mgr, shs_ourst_aft_prosp, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 154 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if duplicates\n",
    "ipo[ipo.astype(str).duplicated()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of actual features\n",
    "len(ipo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "nb_missing_values = sum(map(any, ipo.isnull()))\n",
    "nb_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count missing \n",
    "features_with_nan = len(ipo) - ipo.count()\n",
    "features_with_nan=features_with_nan[features_with_nan!=0]\n",
    "nan_percentage=(features_with_nan/len(ipo)).to_frame('percentage').reset_index().rename(columns={'index':'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pct_int_shs</td>\n",
       "      <td>0.831667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bvps_bef_offer</td>\n",
       "      <td>0.582333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>int_aft</td>\n",
       "      <td>0.604333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>int_bef</td>\n",
       "      <td>0.602333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>intern)shs</td>\n",
       "      <td>0.831667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>pb_value</td>\n",
       "      <td>0.583667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  percentage\n",
       "10     pct_int_shs    0.831667\n",
       "25  bvps_bef_offer    0.582333\n",
       "42         int_aft    0.604333\n",
       "43         int_bef    0.602333\n",
       "44      intern)shs    0.831667\n",
       "62        pb_value    0.583667"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_nan_percentage = nan_percentage[nan_percentage.percentage>0.5]\n",
    "features_nan_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop column with more than 80% of missing values\n",
    "list_to_append = features_nan_percentage.feature.values\n",
    "for col in list_to_append:\n",
    "    columns_to_drop.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('float64'), dtype('<M8[ns]'), dtype('int64')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check type of values\n",
    "ipo.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devide dataframe in function of type.\n",
    "ints = ipo.select_dtypes(include='int64')\n",
    "floats = ipo.select_dtypes(include='float64')\n",
    "objects = ipo.select_dtypes(include='object')\n",
    "timestamps = ipo.select_dtypes(include='M8[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = ['amd_date', 'lockup_date', 'lockup_days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Pre-processsing of Special Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the df\n",
    "ipo_processing = ipo.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split it into 2 one numerical and second categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5000000.0 Common Shares\n",
       "1    6898541.0 Common Shares\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo_processing['description'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5000000,  6898541,  2700000, ...,  2816750, 18779865,  5781126],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the ',' in the features description\n",
    "ipo_processing['description'] = ipo['description'].str.replace(',','',regex=False)\n",
    "# split description into 2 string\n",
    "split_description = ipo_processing.description.str.split('.0 ')\n",
    "# create desctiption_numeric as the nb written in description \n",
    "ipo_processing['description_numeric']= split_description.apply(lambda x: int(x[0]))\n",
    "# create description cat as the category of the number of shares\n",
    "ipo_processing['description_cat'] =  split_description.apply(lambda x: x[1])\n",
    "\n",
    "ipo_processing['description_numeric'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop description\n",
    "columns_to_drop.append('description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all_sic : COMMENT THE CELL BY EXPLAINING WHATS HAPPENING INSIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['8093', '6282/6722/6726/6799/6719/6351', '5945/5199/6719/5999',\n",
       "       ..., '7372/7319/7311', '3111/3172', '6519/6531/6162'], dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo_processing.all_sic.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to reduce this number by a 2 digit categories for all_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.all_sic = ipo.all_sic.str.split('/')\n",
    "\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x: [a[:2] for a in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this file to reduce the number of categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sic_mapping = pd.read_excel(DATA_FOLDER + '/all_sic_mapping.xlsx',delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>range_sic</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0100-0999</td>\n",
       "      <td>Agriculture, Forestry and Fishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000-1499</td>\n",
       "      <td>Mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500-1799</td>\n",
       "      <td>Construction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1800-1999</td>\n",
       "      <td>not used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-3999</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   range_sic                                cat\n",
       "0  0100-0999  Agriculture, Forestry and Fishing\n",
       "1  1000-1499                             Mining\n",
       "2  1500-1799                       Construction\n",
       "3  1800-1999                           not used\n",
       "4  2000-3999                      Manufacturing"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sic_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning file:\n",
    "all_sic_mapping.range_sic = all_sic_mapping.range_sic.str.split('-')\n",
    "\n",
    "all_sic_mapping.range_sic= all_sic_mapping.range_sic.apply(lambda x: [int(a[:2]) for a in x])\n",
    "\n",
    "all_sic_mapping.range_sic = all_sic_mapping.range_sic.apply(lambda x: np.arange(x[0],x[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>range_sic</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>Agriculture, Forestry and Fishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10, 11, 12, 13, 14]</td>\n",
       "      <td>Mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[15, 16, 17]</td>\n",
       "      <td>Construction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[18, 19]</td>\n",
       "      <td>not used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           range_sic  \\\n",
       "0                        [1, 2, 3, 4, 5, 6, 7, 8, 9]   \n",
       "1                               [10, 11, 12, 13, 14]   \n",
       "2                                       [15, 16, 17]   \n",
       "3                                           [18, 19]   \n",
       "4  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...   \n",
       "\n",
       "                                 cat  \n",
       "0  Agriculture, Forestry and Fishing  \n",
       "1                             Mining  \n",
       "2                       Construction  \n",
       "3                           not used  \n",
       "4                      Manufacturing  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sic_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map all_sic values to new ones\n",
    "splitted = all_sic_mapping.range_sic.apply(pd.Series).stack().reset_index(level = 1,drop = True).to_frame('range')\n",
    "\n",
    "merged = pd.merge(splitted,all_sic_mapping,left_on = splitted.index, right_on=  all_sic_mapping.index,how = 'left')\n",
    "\n",
    "merged_2 = merged[['range','cat']]\n",
    "\n",
    "merged_2.range = merged_2.range.astype(int)\n",
    "\n",
    "merged_2.set_index('range',inplace = True)\n",
    "\n",
    "\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x : [merged_2.loc[int(a)] if (int(a) in merged_2.index.values) else int(a) for a in x ] )\n",
    "\n",
    "ipo_processing.all_sic = ipo_processing.all_sic.apply(lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing = pd.concat([ipo_processing,pd.get_dummies(ipo_processing.all_sic.apply(pd.Series).stack()).sum(level=0)],axis = 1)\n",
    "columns_to_drop.append('all_sic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip : done\n",
    "We will use this file 'us_postal_codes.csv' to get cities and state to replace where there is missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo['state'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1507"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ipo['zip'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zip          state\n",
       "0   501       New York\n",
       "1   544       New York\n",
       "2  1001  Massachusetts\n",
       "3  1002  Massachusetts\n",
       "4  1003  Massachusetts"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_zip = pd.read_csv(DATA_FOLDER + '/us_postal_codes.csv', sep=',')\n",
    "us_zip.rename(columns={'Zip Code': 'zip', 'State':'state'}, inplace=True)\n",
    "# Drop infos that we don't want\n",
    "us_zip = us_zip.drop(columns=['Place Name', 'State Abbreviation', 'County', 'Latitude', 'Longitude'])\n",
    "us_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge State in function of the ZIP code\n",
    "ipo_processing.zip = ipo.zip.apply(lambda x: str(x)[:5])\n",
    "ipo_processing.zip = ipo_processing.zip.apply(lambda x: int(x) if (str.isdigit(x)) else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipo_processing['zip'] = ipo_processing.apply(lambda x: int(str(x['zip'])[:5]) if ( str(x['nation']) == 'United States' ) else 0 , axis=1)\n",
    "ipo_processing = ipo_processing.merge(us_zip, how='left')\n",
    "ipo_processing.state.fillna('Outside US', inplace=True)\n",
    "#Remove ZIP.\n",
    "\n",
    "columns_to_drop.append('zip')\n",
    "#ipo_processing.drop(columns=['zip'],inplace = True)\n",
    "len(ipo_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Pre-processing of Categorical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### issuer : done DROP ABoVE ???\n",
    "\n",
    "issuer not relevant : ipo.naic_primary contient des fois des lettres? a supprimé?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ipo['issuer']) == len(ipo.issuer.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop.append('issuer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ipo_1.naic_primary BBBBBA  contient des caractères je ne sais psa si c'est faux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing column description_cat\n",
      "#nan :0\n",
      "processing column state\n",
      "#nan :0\n",
      "processing column auditor\n",
      "#nan :8\n",
      "processing column city\n",
      "#nan :6\n",
      "processing column ind_group\n",
      "#nan :0\n",
      "processing column veic_descr\n",
      "#nan :186\n",
      "processing column naic_primary\n",
      "#nan :0\n",
      "processing column naic_decr\n",
      "#nan :1\n",
      "processing column public_descr\n",
      "#nan :0\n",
      "processing column lockup_flag\n",
      "#nan :0\n",
      "processing column sic_main\n",
      "#nan :0\n",
      "processing column nation\n",
      "#nan :0\n",
      "processing column lbo\n",
      "#nan :0\n",
      "processing column prim_naic\n",
      "#nan :0\n",
      "processing column prim_uop\n",
      "#nan :0\n",
      "processing column pe_backed\n",
      "#nan :0\n",
      "processing column shs_out_after\n",
      "#nan :0\n",
      "processing column vc\n",
      "#nan :0\n",
      "processing column mgrs_role\n",
      "#nan :0\n",
      "processing column mgrs\n",
      "#nan :0\n",
      "processing column ht_ind_gr\n",
      "#nan :2\n",
      "processing column ht_ind\n",
      "#nan :2\n",
      "processing column uop\n",
      "#nan :0\n",
      "processing column legal\n",
      "#nan :0\n",
      "processing column exch\n",
      "#nan :0\n",
      "processing column br\n",
      "#nan :0\n",
      "Processing column industry\n",
      "#nan :0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "to_dummies = ['description_cat','state','auditor','city','ind_group','veic_descr','naic_primary','naic_decr','public_descr','lockup_flag','sic_main','nation','lbo','prim_naic','prim_uop','pe_backed','shs_out_after','vc']\n",
    "\n",
    "for col in to_dummies:\n",
    "    print('processing column ' + col)\n",
    "    ipo_processing,columns_to_drop = process_cat_columns(ipo_processing,col,columns_to_drop)\n",
    "\n",
    "columns_with_n = ['mgrs_role','mgrs','ht_ind_gr','ht_ind','uop','legal','exch','br']\n",
    "for col in columns_with_n:\n",
    "    print('processing column ' + col)\n",
    "    ipo_processing,columns_to_drop = process_categorical_with_sep(ipo_processing,col,'\\n',columns_to_drop)\n",
    "\n",
    "print('Processing column industry')\n",
    "ipo_processing,columns_to_drop = process_categorical_with_sep(ipo_processing,'industry','/',columns_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Pre-processing of Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price_range : Ordinal \n",
    "Since Price_range is an ordinal feature we will map it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Above range', 'Below range', 'Within range', nan], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_col = ['price_range']\n",
    "\n",
    "ipo_processing.price_range.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "#nan :15\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "price_range = ipo['price_range']\n",
    "\n",
    "print(len(price_range.unique()))\n",
    "price_range = replace_nan(price_range,'nan')\n",
    "print(len(price_range.unique()))\n",
    "\n",
    "dict_p = {'Above range': 3,\n",
    "          'Within range':2,\n",
    "          'Below range':1, \n",
    "            np.nan: 0}\n",
    "\n",
    "ipo_processing.price_range = ipo_processing.price_range.replace(dict_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Pre-processing of Dates Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amd_date : Done\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate number of amendment\n",
    "ipo_processing['amd_nbr'] = ipo_processing['amd_date'].apply(lambda x:  str(x).count('\\n')+1 if( str(x).count('\\n') > 0) else ( 1 if(len(str(x)) > 0) else 0 ) )\n",
    "num_col.append('amd_nbr')\n",
    "columns_to_drop.append('amd_nbr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lockup_date, lockup_days : Done\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526\n",
      "527\n",
      "Int64Index([   7,    8,    9,   13,   18,   20,   23,   29,   37,   38,\n",
      "            ...\n",
      "            2946, 2953, 2959, 2963, 2964, 2970, 2971, 2987, 2988, 2989],\n",
      "           dtype='int64', length=526)\n",
      "Int64Index([   7,    8,    9,   13,   18,   20,   23,   29,   37,   38,\n",
      "            ...\n",
      "            2946, 2953, 2959, 2963, 2964, 2970, 2971, 2987, 2988, 2989],\n",
      "           dtype='int64', length=527)\n"
     ]
    }
   ],
   "source": [
    "#Check number of missing values\n",
    "print(ipo['lockup_days'].isna().sum())\n",
    "print(ipo['lockup_date'].isna().sum())\n",
    "\n",
    "#Check if missing values are the same\n",
    "print(ipo.loc[ipo['lockup_days'].isna()].index)\n",
    "print(ipo.loc[ipo['lockup_date'].isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nan :562\n"
     ]
    }
   ],
   "source": [
    "#Get days only last \n",
    "ipo_processing['lockup_days'] = ipo['lockup_days'].apply(lambda x: str(x) if( pd.isnull(x) ) else str(x)[-3:] )\n",
    "#Cast it as a number\n",
    "ipo_processing['lockup_days'] = ipo_processing['lockup_days'].apply(lambda x: int(x) if( str(x).isdigit() ) else np.nan )\n",
    "#replace missing values\n",
    "ipo_processing['lockup_days'] = replace_nan(ipo_processing['lockup_days'], '')\n",
    "\n",
    "#ipo_processing['lockup_days'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove lockup_date because this information is rundandant in the lockup days we can calculate from the frist trade date\n",
    "columns_to_drop.append('lockup_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### offer_date, first_trade_date, issue_date : DONE\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "#Compare number of missing value offer_date and issue_date ares nearly the same.\n",
    "print(ipo['issue_date'].isna().sum() )\n",
    "print(ipo['offer_date'].isna().sum() )\n",
    "print(ipo['first_trade_date'].isna().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop first_trade_date\n",
    "columns_to_drop.append('first_trade_date')\n",
    "columns_to_drop.append('issue_date')\n",
    "columnWeTransform = 'offer_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer_date</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>12.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984-02-01</td>\n",
       "      <td>12.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  offer_date   rate\n",
       "0 1984-01-01  12.89\n",
       "1 1984-02-01  12.73"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dataset of the american 10 year bond rate\n",
    "# This give us more info on the actual risk premium\n",
    "rate = pd.read_csv(DATA_FOLDER + '/HQMCB10YR.csv')\n",
    "rate.rename(columns = {'DATE':columnWeTransform, 'HQMCB10YR':'rate'}, inplace = True)\n",
    "rate[columnWeTransform] = rate[columnWeTransform].apply(lambda x: pd.to_datetime(x))\n",
    "rate.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge rate on the dataframe\n",
    "ipo_processing[columnWeTransform] = ipo[columnWeTransform].apply(lambda x: pd.to_datetime(str(x.year)+'-'+str(x.month)+'-01') if( not(pd.isna(x))) else pd.to_datetime('1900-01-01'))\n",
    "ipo_processing = ipo_processing.merge(rate, how='left')\n",
    "columns_to_drop.append(columnWeTransform)\n",
    "num_col.append('rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nan :0\n"
     ]
    }
   ],
   "source": [
    "#Replace missing value rate\n",
    "ipo_processing.rate = replace_nan(ipo_processing.rate, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nan :0\n"
     ]
    }
   ],
   "source": [
    "#Replace missing value by the most frequent one.\n",
    "ipo_processing[columnWeTransform] = ipo[columnWeTransform].apply(lambda x: str(x)[0:10])\n",
    "ipo_processing[columnWeTransform] = replace_nan(ipo[columnWeTransform], '1900-01-01')\n",
    "ipo_processing[columnWeTransform] = ipo_processing[columnWeTransform].apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get datetime object for each row\n",
    "ipo_processing[columnWeTransform+'_day'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.day) )\n",
    "cat_col.append(columnWeTransform+'_day')\n",
    "ipo_processing[columnWeTransform+'_weekday'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.weekday()) )\n",
    "cat_col.append(columnWeTransform+'_weekday')\n",
    "ipo_processing[columnWeTransform+'_month'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.month) )\n",
    "cat_col.append(columnWeTransform+'_month')\n",
    "ipo_processing[columnWeTransform+'_year'] = ipo_processing[columnWeTransform].apply(lambda x: int(x.year) )\n",
    "cat_col.append(columnWeTransform+'_year')\n",
    "ipo_processing[columnWeTransform+'_timestamp'] = ipo_processing[columnWeTransform].apply(lambda x: dt.datetime(year=int(x.year), month=int(x.month), day=int(x.day)).timestamp())\n",
    "num_col.append(columnWeTransform+'_timestamp')\n",
    "\n",
    "#hot encode category\n",
    "ipo_processing = pd.get_dummies(ipo_processing,  columns=[columnWeTransform+'_year',columnWeTransform+'_month',columnWeTransform+'_day',columnWeTransform+'_weekday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date_amd : DONE\n",
    "@Guillaume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nan :7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   2014-09-22\n",
       "Name: date_amd, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert from excel date to datetime\n",
    "ipo_processing['date_amd'] = replace_nan(ipo['date_amd'], '')\n",
    "ipo_processing['date_amd'] = ipo_processing['date_amd'].apply(lambda x: from_excel_ordinal(x) )\n",
    "ipo_processing['date_amd'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate difference between date_amd and offer_date save number of days as int\n",
    "ipo_processing['offer_date'] = ipo['offer_date'].apply(lambda x: pd.to_datetime(x) )\n",
    "ipo_processing['time_amd'] = ipo_processing.apply(lambda x: x['offer_date']-x['date_amd'], axis=1 ) #-x['offer_date'])\n",
    "num_col.append('time_amd')\n",
    "ipo_processing['time_amd'] = ipo_processing['time_amd'].apply(lambda x: x.days)\n",
    "\n",
    "#column to drop:\n",
    "columns_to_drop.append('date_amd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offering_Price: the price at which a company sells its shares to investors.\n",
    "\n",
    "Num_Shares: the total number of outstanding shares.\n",
    "\n",
    "Closing_Price: (at the end of the first day of training) price at which shares trade in the open market, measured at the end of the first day of trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 Pre-processing of Numerical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by numerical features that needs more preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum over all values in round_tot\n",
    "ipo_processing.round_tot = ipo_processing.round_tot.fillna(0)\n",
    "ipo_processing.round_tot = ipo_processing.round_tot.replace(',','',regex = True)\n",
    "ipo_processing.round_tot = ipo_processing.round_tot.apply(lambda x: x.split('\\n') if str(x).isdigit()==False else [x] )\n",
    "\n",
    "ipo_processing.round_tot = [sum([float(x) for x in j if re.match(\"^\\d+?\\.\\d+?$\", str(x)) or str(x).isdigit()])   for j in ipo_processing.round_tot.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing['mgt_fee'] =ipo_processing.mgt_fee.replace('Comb.', np.nan)\n",
    "ipo_processing['gross_spread'] = ipo_processing.gross_spread.replace('na', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace ' in some numerical features:\n",
    "col_to_replace = ['min_round_vexp','avg_round_vexp', 'max_round_vexp', 'min_firm_amt_vexp', 'avg_firm_amt_vexp', 'max_firm_amt_vexp',\\\n",
    "          'min_fund_amt_vexp', 'max_fund_amt_vexp']\n",
    "for col in col_to_replace:\n",
    "    ipo_processing[col] = ipo_processing[col].str.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill null values \n",
    "for col in num_col:\n",
    "    ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing_Price\n",
      "Offering_Price\n",
      "pctchg_dj_1\n",
      "pctchg_hp\n",
      "pctchg_lp\n",
      "pctchg_mp\n",
      "pctchg_nasdaq_1\n",
      "pctchg_sp_1\n",
      "pctchg_sp_2\n",
      "pctchg_dj_2\n",
      "pctchg_nasdaq_2\n",
      "pctchg_p\n",
      "pctchg_sp_3\n",
      "pctchg_sp_4\n",
      "pctchg_amd_amt\n",
      "pct_ins_shs_aft\n",
      "pct_ins_shs_bef\n",
      "mkt_cap\n",
      "acc_fee\n",
      "amd_hp\n",
      "amd_lp\n",
      "amd_mp\n",
      "amd_pr_shs_pct\n",
      "amd_pr_amt\n",
      "amd_pr_shst_tot\n",
      "amd_shst_tot\n",
      "amt_filed\n",
      "bluesky\n",
      "book_proceeds\n",
      "book_proceeds_ovt\n",
      "bvps\n",
      "comm_eq\n",
      "comm_eq_bef\n",
      "dj_avg_1\n",
      "dj_avg_2\n",
      "date_found\n",
      "days_in_registr\n",
      "deal_size\n",
      "exp_pctofproceeds\n",
      "exp_incl_gross\n",
      "filing_date\n",
      "free_float\n",
      "gross_spread_allmkt\n",
      "lastamd_tot\n",
      "legal_exp\n",
      "mktval_aft\n",
      "mktval_bef\n",
      "misc_exp\n",
      "h_fil_p\n",
      "l_fil_p\n",
      "m_fil_p\n",
      "nasdaq_avg_1\n",
      "nasdaq_avg_2\n",
      "num_emp\n",
      "num_emp_date\n",
      "shs_ourst_aft_prosp\n",
      "ord_shs\n",
      "prim_shs_1\n",
      "prim_shs_2\n",
      "prim_amt\n",
      "prim_shs_3\n",
      "print_exp\n",
      "proceeds_1\n",
      "proceeds_2\n",
      "proceeds_3\n",
      "SP1\n",
      "SP2\n",
      "SP3\n",
      "SP4\n",
      "SEC_fee\n",
      "shs_out_bef\n",
      "tang_ce_1\n",
      "tang_ce_2\n",
      "tor_ovt\n",
      "reall_fee\n",
      "tot_amt\n",
      "tot_ass_after\n",
      "tot ass_before\n",
      "tot_inv\n",
      "tot_mgtfee\n",
      "underw_fee\n",
      "num_rounds_vexp\n",
      "num_firms_vexp\n",
      "num_funds_vexp\n"
     ]
    }
   ],
   "source": [
    "floats = ipo.select_dtypes(include='float64')\n",
    "\n",
    "for col in floats.columns:\n",
    "    if not(col in columns_to_drop):\n",
    "        print(col)\n",
    "        ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ints.columns:\n",
    "     ipo_processing[col] = DataFrameImputer().fit_transform(ipo_processing[[col]].astype(np.float32))[col]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7 Pre-processing of Textual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk_Factors :  Textual description of all the risk / done \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-14d4f55b0055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrisks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mipo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Risk_Factors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrisks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_text_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrisks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\dob_final_project\\preprocessing_functions.py\u001b[0m in \u001b[0;36mprocess_text_columns\u001b[1;34m(risks)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m# Remove stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "risks = ipo['Risk_Factors']\n",
    "\n",
    "risks = process_text_columns(risks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF --------------------------------------------------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 500)\n",
    "risks_tfidf = vectorizer.fit_transform(risks)\n",
    "risks_tfidf = risks_tfidf.toarray()\n",
    "\n",
    "#Convert to a dataframe\n",
    "risks_tfidf = pd.DataFrame(risks_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnToDrop.append('Risk_Factors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check for the Nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columnToDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.drop(columns,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Visualize pattern of missing data\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(ipo_processing.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.columns[ipo_processing.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.9 : TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing['Target'] = ipo_processing.Closing_Price >= ipo_processing.Offering_Price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing.Closing_Price == ipo_processing.Offering_Price )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipo_processing['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_processing.Target = ipo_processing.Target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ipo_processing.Target.value_counts()\n",
    "\n",
    "sns.set()\n",
    "\n",
    "ones = counts[1]/len(counts) * 100\n",
    "zeros = counts[0]/len(counts) * 100\n",
    "labels = 'Price increased', 'Price decreased'\n",
    "plt.pie( [ones, zeros], labels=labels, autopct='%1.2f%%', startangle=360)\n",
    "plt.title('Distribution of target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour le preprocessing Je pense qu'il manque: Num_Shares, lockup_days, lockup_date Mais Num_shares est-ce qu'on l'utilise pour le target et donc pas pour les features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7 Features Reduction\n",
    "\n",
    "### 1. Check the correlation with the target\n",
    "    -> Drop bottom 5%\n",
    "### 2. Check the correlation between feature\n",
    "    -> Drop \n",
    "    \n",
    "### 0.7.1. Check the correlation with the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('float64'), dtype('<M8[ns]'), dtype('int64'),\n",
       "       dtype('uint8')], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check type of values\n",
    "ipo_processing.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                                       Risk_Factors mgt_fee  \\\n",
      "0     000307  >risk factors \\ninvesting in our common stock ...   Comb.   \n",
      "1     000833  risk factors an investment in our common stock...   Comb.   \n",
      "2     00086T  RISK FACTORS In addition to the other informat...    0.21   \n",
      "3     00087X  RISK FACTORS An investment in the shares of Co...    0.12   \n",
      "4     000881  RISK FACTORS In addition to the other informat...    0.17   \n",
      "5     001228  RISK FACTORS \\nAn investment in our common sto...   Comb.   \n",
      "6     001296  RISK FACTORS In addition to the other informat...    0.14   \n",
      "7     00163T  RISK FACTORS An investment in the shares of Co...    0.26   \n",
      "8     001744  RISK FACTORS An investment in our common stock...    0.24   \n",
      "9     001944  \\n                                  RISK FACTO...    0.21   \n",
      "10    00206F  RISK FACTORS In addition to the other informat...    0.09   \n",
      "11    00206P  RISK FACTORS In addition to the other informat...    0.13   \n",
      "12    00207W  RISK FACTORS You should consider carefully the...    0.11   \n",
      "13    00208J  >risk factors    you should read\\ncarefully th...    0.19   \n",
      "14    002120  risk factors investing in our common stock inv...   Comb.   \n",
      "15    002121  risk factors investing in our common stock inv...    0.21   \n",
      "16    00253G  risk factors you should carefully consider the...    0.12   \n",
      "17    00253U  RISK FACTORS In addition to the other informat...     0.1   \n",
      "18    002896  \\n \\n                                 RISK FAC...    0.22   \n",
      "19    00339B  RISK FACTORS This Prospectus contains forward-...     0.1   \n",
      "20    003709  RISK FACTORS PROSPECTIVE INVESTORS SHOULD CARE...    0.04   \n",
      "21    003743  RISK FACTORS An investment in the shares of Co...    0.19   \n",
      "22    004225  risk factors before investing in our common st...   0.098   \n",
      "23    00429P  RISK FACTORS An investment in our common stock...    0.21   \n",
      "24    004305  RISK FACTORS An investment in the shares of Co...    0.11   \n",
      "25    00430L  risk factors investing in our common stock inv...   Comb.   \n",
      "26    00430P  RISK FACTORS In addition to the other informat...    0.13   \n",
      "27    00434H  risk factors investing in our common stock inv...   Comb.   \n",
      "28    00437V  RISK FACTORS THE COMMON STOCK OFFERED HEREBY I...    0.23   \n",
      "29    00437W  RISK FACTORS You should carefully consider the...    0.13   \n",
      "...      ...                                                ...     ...   \n",
      "2970  896775  RISK FACTORS Any investment in our common stoc...    0.21   \n",
      "2971  89677M  RISK FACTORS In addition to the other informat...    0.25   \n",
      "2972  89678F  >risk factors \\ninvesting in our common stock ...   Comb.   \n",
      "2973  896818  RISK FACTORS IN ADDITION TO THE OTHER INFORMAT...    0.27   \n",
      "2974  89685A  risk factors investing in our common stock inv...   Comb.   \n",
      "2975  89685K  risk factors before you decide to invest in ou...   Comb.   \n",
      "2976  896882     9\\n                                  RISK F...    0.12   \n",
      "2977  897051  risk factors an investment in our classa commo...    0.18   \n",
      "2978  89733N  RISK FACTORS YOU SHOULD CAREFULLY CONSIDER THE...     0.1   \n",
      "2979  89778N  risk factors you should carefully consider the...   Comb.   \n",
      "2980  897868  RISK FACTORS An investment in our common stock...    0.07   \n",
      "2981  897888  risk factors investing in our common stock inv...   Comb.   \n",
      "2982  897908  RISK FACTORS In addition to the other informat...    0.19   \n",
      "2983  898202  risk factors investing in our common stock inv...   Comb.   \n",
      "2984  898452  RISK FACTORS    Investing in our\\ncommon stock...   Comb.   \n",
      "2985  89853L  risk factors investing in our common stock inv...   Comb.   \n",
      "2986  898570  risk factors investing in our common stock inv...     0.1   \n",
      "2987  899035  \\n                                  RISK FACTO...    0.21   \n",
      "2988  899165  RISK FACTORS An investment in our common stock...     0.2   \n",
      "2989  899690  RISK FACTORS ANY INVESTMENT IN OUR COMMON STOC...    0.17   \n",
      "2990  89969Q  >risk factors \\ninvesting in our common stock ...   0.216   \n",
      "2991  8A0198  risk factors investing in our classa common st...   0.255   \n",
      "2992  8A2650  risk factors an investment in our common stock...   Comb.   \n",
      "2993  8A2699  RISK FACTORS We are subject to various risks t...   Comb.   \n",
      "2994  8A3659  > \\n \\n  risk factors     \\n        before you...   Comb.   \n",
      "2995  8A3660  risk factors investing in our common stock inv...   Comb.   \n",
      "2996  8A3897  risk factors investing in our common stock inv...    0.17   \n",
      "2997  8A6188  risk factors investing in our common stock inv...   Comb.   \n",
      "2998  8A6646  risk factors investing in our common stock inv...   Comb.   \n",
      "2999  8A7067  >risk factors \\nan investment in our common st...   Comb.   \n",
      "\n",
      "          exch                                          mgrs_role  \\\n",
      "0     New York   JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAGER   \n",
      "1     New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2       Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "3       Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "4       Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "5     New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "6       Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "7     New York  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGE...   \n",
      "8     New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAG...   \n",
      "9     American  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "10      Nasdaq  BOOK RUNNER\\nSYNDICATE MEMBER\\nSYNDICATE MEMBE...   \n",
      "11      Nasdaq  BOOK RUNNER\\nSYNDICATE MEMBER\\nSYNDICATE MEMBE...   \n",
      "12      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGE...   \n",
      "13      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGE...   \n",
      "14      Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT LE...   \n",
      "15    New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "16    New York  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "17      Nasdaq                            BOOK RUNNER\\nCO MANAGER   \n",
      "18    New York  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGE...   \n",
      "19      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "20      Nasdaq                            BOOK RUNNER\\nCO MANAGER   \n",
      "21      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "22      Nasdaq  BOOK RUNNER\\nJOINT LEAD MANAGER\\nCO MANAGER\\nC...   \n",
      "23      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "24      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "25      Nasdaq    BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGER   \n",
      "26      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "27      Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT LE...   \n",
      "28      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "29      Nasdaq  BOOK RUNNER\\nCO MANAGER\\nSYNDICATE MEMBER\\nSYN...   \n",
      "...        ...                                                ...   \n",
      "2970    Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "2971    Nasdaq  BOOK RUNNER\\nJOINT LEAD MANAGER\\nCO MANAGER\\nC...   \n",
      "2972    Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2973  New York                            BOOK RUNNER\\nCO MANAGER   \n",
      "2974    Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAG...   \n",
      "2975    Nasdaq    BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGER   \n",
      "2976    Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "2977  New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAG...   \n",
      "2978    Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "2979    Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGE...   \n",
      "2980    Nasdaq                BOOK RUNNER\\nCO MANAGER\\nCO MANAGER   \n",
      "2981  New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAG...   \n",
      "2982    Nasdaq                BOOK RUNNER\\nCO MANAGER\\nCO MANAGER   \n",
      "2983  New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2984    Nasdaq    BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGER   \n",
      "2985    Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAG...   \n",
      "2986    Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2987    Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGE...   \n",
      "2988    Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nCO MANAGE...   \n",
      "2989    Nasdaq  BOOK RUNNER\\nCO MANAGER\\nCO MANAGER\\nSYNDICATE...   \n",
      "2990  New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2991  New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2992    Nasdaq               JOINT BOOK RUNNER\\nJOINT BOOK RUNNER   \n",
      "2993    Nasdaq                                        BOOK RUNNER   \n",
      "2994  NYSE MKT  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2995    Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAG...   \n",
      "2996    Nasdaq  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "2997    Nasdaq                                        BOOK RUNNER   \n",
      "2998    Nasdaq   JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nCO MANAGER   \n",
      "2999  New York  JOINT BOOK RUNNER\\nJOINT BOOK RUNNER\\nJOINT BO...   \n",
      "\n",
      "                                                   mgrs  \\\n",
      "0     William Blair & Co\\nRaymond James & Associates...   \n",
      "1     Credit Suisse\\nJP Morgan\\nBear Stearns & Co In...   \n",
      "2     BT Alex Brown Inc\\nJanney Montgomery Scott Inc...   \n",
      "3     JC Bradford & Co\\nJanney Montgomery Scott Inc\\...   \n",
      "4     Robinson-Humphrey Co\\nWheat First Butcher & Si...   \n",
      "5     Deutsche Bank Securities\\nStifel Nicolaus & Co...   \n",
      "6     Alex Brown & Sons Inc\\nRobinson-Humphrey Co\\nD...   \n",
      "7     Morgan Stanley Dean Witter\\nBT Alex Brown Inc\\...   \n",
      "8     Banc of America Securities LLC\\nUBS Warburg\\nJ...   \n",
      "9     Morgan Stanley & Co\\nDonaldson Lufkin & Jenret...   \n",
      "10    HC Wainwright & Co Inc\\nMontgomery Securities\\...   \n",
      "11    Van Kasper & Co\\nBT Alex Brown Inc\\nFurman Sel...   \n",
      "12    Bear Stearns & Co Inc\\nPrudential Securities I...   \n",
      "13    Lehman Brothers\\nCIBC World Markets Inc\\nRaymo...   \n",
      "14    JP Morgan\\nCitigroup Global Markets Inc\\nBMO C...   \n",
      "15    Morgan Stanley & Co\\nBank of America Merrill L...   \n",
      "16    Friedman Billings Ramsey Group\\nCredit Suisse ...   \n",
      "17                                     Cowen\\nJP Morgan   \n",
      "18    Goldman Sachs & Co\\nLazard Freres & Co LLC\\nMo...   \n",
      "19    BancAmerica Robertson Stephens\\nLehman Brother...   \n",
      "20    Kashner Davidson Securities\\nAndrew, Alexander...   \n",
      "21    CIBC Oppenheimer\\nVolpe Brown Whelan & Co\\nHam...   \n",
      "22    Banc of America Securities LLC\\nPiper Jaffray ...   \n",
      "23    CS First Boston Corp\\nUBS Warburg\\nUS Bancorp ...   \n",
      "24    Cowen\\nJanney Montgomery Scott Inc\\nBear Stear...   \n",
      "25    Jefferies & Co Inc\\nFerris Baker Watts\\nStifel...   \n",
      "26    Cowen\\nRobertson Stephens & Co\\nSoundview Fina...   \n",
      "27    Citigroup Global Markets Inc\\nLeerink Swann & ...   \n",
      "28    Hambrecht & Quist\\nNationsBanc Montgomery Sec\\...   \n",
      "29    BancBoston Robertson Stephens\\nThomas Weisel P...   \n",
      "...                                                 ...   \n",
      "2970  CS First Boston Corp\\nDeutsche Banc Alex Brown...   \n",
      "2971  Morgan Stanley Dean Witter\\nLehman Brothers\\nS...   \n",
      "2972  Stephens Inc\\nKeefe Bruyette & Woods Inc\\nRobe...   \n",
      "2973       Alex Brown & Sons Inc\\nDillon, Read & Co Inc   \n",
      "2974  JP Morgan\\nCredit Suisse Securities (USA)\\nCan...   \n",
      "2975  Citi\\nPiper Jaffray Inc\\nCanaccord Genuity\\nJM...   \n",
      "2976  Bear Stearns & Co Inc\\nDonaldson Lufkin & Jenr...   \n",
      "2977  Lehman Brothers\\nJP Morgan\\nCitigroup\\nCredit ...   \n",
      "2978  Cruttenden Roth Inc\\nPennsylvania Merchant Gro...   \n",
      "2979  Morgan Stanley\\nBanc of America Securities LLC...   \n",
      "2980  CE Unterberg Towbin\\nCruttenden Roth Inc\\nPenn...   \n",
      "2981  JP Morgan\\nDeutsche Bank Securities Corp\\nRBC ...   \n",
      "2982  JP Morgan\\nDonaldson Lufkin & Jenrette\\nFurman...   \n",
      "2983  RBC Capital Markets\\nBarclays\\nStifel Nicolaus...   \n",
      "2984  Credit Suisse First Boston\\nThomas Weisel Part...   \n",
      "2985  Goldman Sachs & Co\\nMorgan Stanley & Co\\nLeeri...   \n",
      "2986  Bank of America Merrill Lynch\\nCitigroup Globa...   \n",
      "2987  Merrill Lynch\\nBT Alex Brown Inc\\nWilliam Blai...   \n",
      "2988  Lehman Brothers\\nHambrecht & Quist\\nWarburg Di...   \n",
      "2989  CS First Boston Corp\\nHambrecht & Quist\\nING B...   \n",
      "2990  Goldman Sachs & Co\\nCredit Suisse\\nJP Morgan\\n...   \n",
      "2991  Deutsche Bank Securities\\nCitigroup\\nWells Far...   \n",
      "2992  Sandler O'Neill Partners\\nKeefe Bruyette & Woo...   \n",
      "2993                                   MDB Capital Corp   \n",
      "2994  Robert W Baird & Co Inc\\nBMO Capital Markets\\n...   \n",
      "2995  BMO Capital Markets\\nWedbush Securities\\nCanto...   \n",
      "2996  Merrill Lynch, Pierce, Fenner\\nJefferies & Co ...   \n",
      "2997                                   MDB Capital Corp   \n",
      "2998       Jefferies LLC\\nCowen & Co\\nCanaccord Genuity   \n",
      "2999  Citi\\nJP Morgan\\nCredit Suisse\\nZelman Partner...   \n",
      "\n",
      "                                                all_sic  \\\n",
      "0                                            [Services]   \n",
      "1     [Finance, Insurance and Real Estate, Finance, ...   \n",
      "2     [Retail Trade, Wholesale Trade, Finance, Insur...   \n",
      "3                                            [Services]   \n",
      "4     [Services, Services, Services, Services, Servi...   \n",
      "5                  [Finance, Insurance and Real Estate]   \n",
      "6                                  [Services, Services]   \n",
      "7     [Finance, Insurance and Real Estate, Finance, ...   \n",
      "8                                  [Services, Services]   \n",
      "9     [Public Administration, Finance, Insurance and...   \n",
      "10                                           [Services]   \n",
      "11    [Transportation, Communications, Electric, Gas...   \n",
      "12                                 [Services, Services]   \n",
      "13                             [Mining, Mining, Mining]   \n",
      "14                                      [Manufacturing]   \n",
      "15                                           [Services]   \n",
      "16                 [Finance, Insurance and Real Estate]   \n",
      "17        [Manufacturing, Services, Services, Services]   \n",
      "18                         [Retail Trade, Retail Trade]   \n",
      "19                                           [Services]   \n",
      "20    [Finance, Insurance and Real Estate, Finance, ...   \n",
      "21    [Services, Services, Transportation, Communica...   \n",
      "22        [Manufacturing, Services, Services, Services]   \n",
      "23                       [Manufacturing, Manufacturing]   \n",
      "24                       [Manufacturing, Manufacturing]   \n",
      "25        [Manufacturing, Services, Services, Services]   \n",
      "26                                      [Manufacturing]   \n",
      "27        [Manufacturing, Services, Services, Services]   \n",
      "28    [Services, Services, Wholesale Trade, Finance,...   \n",
      "29                                 [Services, Services]   \n",
      "...                                                 ...   \n",
      "2970                                    [Manufacturing]   \n",
      "2971  [Transportation, Communications, Electric, Gas...   \n",
      "2972  [Finance, Insurance and Real Estate, Finance, ...   \n",
      "2973  [Manufacturing, Manufacturing, Manufacturing, ...   \n",
      "2974                   [Manufacturing, Wholesale Trade]   \n",
      "2975                     [Manufacturing, Manufacturing]   \n",
      "2976  [Services, Services, Transportation, Communica...   \n",
      "2977  [Manufacturing, Manufacturing, Manufacturing, ...   \n",
      "2978                                    [Manufacturing]   \n",
      "2979      [Manufacturing, Services, Services, Services]   \n",
      "2980                                    [Manufacturing]   \n",
      "2981  [Services, Services, Finance, Insurance and Re...   \n",
      "2982           [Services, Services, Services, Services]   \n",
      "2983               [Finance, Insurance and Real Estate]   \n",
      "2984  [Services, Services, Services, Services, Servi...   \n",
      "2985                          [Services, Manufacturing]   \n",
      "2986                     [Services, Services, Services]   \n",
      "2987                                     [Retail Trade]   \n",
      "2988                                    [Manufacturing]   \n",
      "2989                                         [Services]   \n",
      "2990                     [Manufacturing, Manufacturing]   \n",
      "2991  [Finance, Insurance and Real Estate, Finance, ...   \n",
      "2992  [Finance, Insurance and Real Estate, Finance, ...   \n",
      "2993                                         [Services]   \n",
      "2994               [Finance, Insurance and Real Estate]   \n",
      "2995                                    [Manufacturing]   \n",
      "2996                                    [Manufacturing]   \n",
      "2997                                    [Manufacturing]   \n",
      "2998                                    [Manufacturing]   \n",
      "2999                                     [Construction]   \n",
      "\n",
      "                                               amd_date  \\\n",
      "0                          08/15/14\\n09/10/14\\n09/22/14   \n",
      "1      06/28/06\\n08/28/06\\n10/03/06\\n10/27/06\\n11/02/06   \n",
      "2                          09/16/97\\n10/07/97\\n10/08/97   \n",
      "3                12/04/97\\n12/17/97\\n01/13/98\\n02/04/98   \n",
      "4                                    07/23/97\\n08/06/97   \n",
      "5     04/05/11\\n04/18/11\\n04/25/11\\n04/26/11\\n06/22/...   \n",
      "6                          19970321\\n19970319\\n19970303   \n",
      "7                10/24/97\\n11/04/97\\n11/14/97\\n11/20/97   \n",
      "8                          08/21/01\\n09/14/01\\n10/19/01   \n",
      "9      03/27/97\\n05/07/97\\n05/23/97\\n06/10/97\\n06/17/97   \n",
      "10                                   03/07/97\\n03/21/97   \n",
      "11               04/01/98\\n04/14/98\\n05/04/98\\n05/06/98   \n",
      "12                                   10/15/99\\n11/08/99   \n",
      "13               12/12/00\\n01/04/01\\n01/12/01\\n01/30/01   \n",
      "14                         04/27/15\\n05/01/15\\n05/05/15   \n",
      "15                                   03/07/14\\n03/13/14   \n",
      "16    05/27/04\\n07/21/04\\n09/03/04\\n09/28/04\\n10/04/...   \n",
      "17    11/18/96\\n12/19/96\\n01/07/97\\n01/28/97\\n01/29/...   \n",
      "18                                   08/23/96\\n08/28/96   \n",
      "19    04/20/98\\n04/22/98\\n05/05/98\\n05/20/98\\n05/26/...   \n",
      "20               11/06/98\\n04/15/99\\n05/17/99\\n05/27/99   \n",
      "21    10/05/98\\n10/21/98\\n11/06/98\\n11/16/98\\n12/07/...   \n",
      "22    04/02/04\\n04/27/04\\n05/05/04\\n05/19/04\\n05/20/...   \n",
      "23     03/28/00\\n04/17/00\\n06/01/00\\n06/21/00\\n06/22/00   \n",
      "24                         10/30/96\\n11/18/96\\n12/09/96   \n",
      "25    04/06/05\\n05/16/05\\n06/10/05\\n06/27/05\\n06/30/...   \n",
      "26                                   03/13/97\\n04/10/97   \n",
      "27                         08/19/13\\n09/06/13\\n09/19/13   \n",
      "28    09/04/98\\n10/20/98\\n01/29/99\\n03/26/99\\n04/07/...   \n",
      "29                                   07/06/99\\n07/20/99   \n",
      "...                                                 ...   \n",
      "2970  03/07/00\\n04/17/00\\n06/06/00\\n06/19/00\\n06/28/...   \n",
      "2971   09/23/99\\n10/12/99\\n10/20/99\\n10/21/99\\n10/27/99   \n",
      "2972                       04/16/13\\n04/23/13\\n05/07/13   \n",
      "2973                       09/26/96\\n10/16/96\\n10/23/96   \n",
      "2974                                 04/03/14\\n04/11/14   \n",
      "2975  12/10/09\\n01/05/10\\n02/09/10\\n02/25/10\\n07/13/...   \n",
      "2976   08/18/99\\n09/14/99\\n09/27/99\\n10/04/99\\n10/06/99   \n",
      "2977  07/29/05\\n09/16/05\\n10/24/05\\n11/09/05\\n11/18/...   \n",
      "2978  06/30/98\\n07/16/98\\n08/04/98\\n04/13/99\\n06/25/...   \n",
      "2979  07/18/06\\n08/18/06\\n09/22/06\\n10/02/06\\n10/05/...   \n",
      "2980                                 12/02/99\\n12/15/99   \n",
      "2981                                 09/06/12\\n09/13/12   \n",
      "2982             07/17/96\\n08/30/96\\n10/04/96\\n10/09/96   \n",
      "2983                                              41827   \n",
      "2984  06/17/05\\n06/24/05\\n07/11/05\\n07/27/05\\n08/04/...   \n",
      "2985                                 07/16/14\\n07/28/14   \n",
      "2986             04/14/14\\n05/01/14\\n07/07/14\\n07/17/14   \n",
      "2987                                 03/29/99\\n04/21/99   \n",
      "2988             10/27/99\\n11/18/99\\n12/07/99\\n12/09/99   \n",
      "2989             06/29/99\\n07/13/99\\n07/28/99\\n08/05/99   \n",
      "2990  01/25/12\\n02/10/12\\n03/15/12\\n04/03/12\\n04/09/...   \n",
      "2991   01/10/14\\n01/14/14\\n01/21/14\\n01/24/14\\n01/28/14   \n",
      "2992             01/27/14\\n03/05/14\\n03/14/14\\n03/25/14   \n",
      "2993                                 03/12/14\\n03/21/14   \n",
      "2994             02/12/14\\n03/11/14\\n03/24/14\\n04/07/14   \n",
      "2995                                 03/04/14\\n03/12/14   \n",
      "2996             09/11/15\\n09/17/15\\n09/28/15\\n10/02/15   \n",
      "2997   03/21/14\\n04/11/14\\n05/16/14\\n05/21/14\\n05/27/14   \n",
      "2998                                              41698   \n",
      "2999  06/26/13\\n07/03/13\\n07/11/13\\n07/16/13\\n07/18/...   \n",
      "\n",
      "                            auditor  \\\n",
      "0                       BDO USA LLP   \n",
      "1             Deloitte & Touche LLP   \n",
      "2                  Price Waterhouse   \n",
      "3              Arthur Andersen & Co   \n",
      "4                 Ernst & Young LLP   \n",
      "5       Pricewaterhouse Coopers LLP   \n",
      "6              Arthur Andersen & Co   \n",
      "7              Arthur Andersen & Co   \n",
      "8                          KPMG LLP   \n",
      "9                 Ernst & Young LLP   \n",
      "10            Coopers & Lybrand LLP   \n",
      "11            Coopers & Lybrand LLP   \n",
      "12                Ernst & Young LLP   \n",
      "13                         KPMG LLP   \n",
      "14                Ernst & Young LLP   \n",
      "15            Deloitte & Touche LLP   \n",
      "16                Ernst & Young LLP   \n",
      "17            Coopers & Lybrand LLP   \n",
      "18            Coopers & Lybrand LLP   \n",
      "19                Ernst & Young LLP   \n",
      "20              Simontacci & Co LLP   \n",
      "21            Deloitte & Touche LLP   \n",
      "22      Pricewaterhouse Coopers LLP   \n",
      "23           PricewaterhouseCoopers   \n",
      "24                 Price Waterhouse   \n",
      "25                Aidman Piser & Co   \n",
      "26                 Price Waterhouse   \n",
      "27                Ernst & Young LLP   \n",
      "28                Ernst & Young LLP   \n",
      "29           PricewaterhouseCoopers   \n",
      "...                             ...   \n",
      "2970              Ernst & Young LLP   \n",
      "2971         PricewaterhouseCoopers   \n",
      "2972                       KPMG LLP   \n",
      "2973              Ernst & Young LLP   \n",
      "2974         PricewaterhouseCoopers   \n",
      "2975              Ernst & Young LLP   \n",
      "2976         PricewaterhouseCoopers   \n",
      "2977              Ernst & Young LLP   \n",
      "2978         McGladrey & Pullen LLP   \n",
      "2979              Ernst & Young LLP   \n",
      "2980         PricewaterhouseCoopers   \n",
      "2981          Deloitte & Touche LLP   \n",
      "2982              Ernst & Young LLP   \n",
      "2983              Ernst & Young LLP   \n",
      "2984          Deloitte & Touche LLP   \n",
      "2985              Ernst & Young LLP   \n",
      "2986                       KPMG LLP   \n",
      "2987          KPMG Peat Marwick LLP   \n",
      "2988              Ernst & Young LLP   \n",
      "2989          KPMG Peat Marwick LLP   \n",
      "2990             Grant Thornton LLP   \n",
      "2991    Pricewaterhouse Coopers LLP   \n",
      "2992             Grant Thornton LLP   \n",
      "2993          Marcum & Kliegman LLP   \n",
      "2994    Pricewaterhouse Coopers LLP   \n",
      "2995         McGladrey & Pullen LLP   \n",
      "2996    Pricewaterhouse Coopers LLP   \n",
      "2997  Squar,Milner,Peterson,Miranda   \n",
      "2998                   Deloitte LLP   \n",
      "2999              Ernst & Young LLP   \n",
      "\n",
      "                                                     br       ...         vc  \\\n",
      "0     William Blair & Co\\nRaymond James & Associates...       ...         No   \n",
      "1     Credit Suisse\\nJP Morgan & Co Inc\\nBear Stearn...       ...         No   \n",
      "2                                     BT Alex Brown Inc       ...         No   \n",
      "3                                      JC Bradford & Co       ...         No   \n",
      "4                                  Robinson-Humphrey Co       ...         No   \n",
      "5     Deutsche Bank Securities Inc\\nStifel Nicolaus ...       ...         No   \n",
      "6                                 Alex Brown & Sons Inc       ...         No   \n",
      "7                       Morgan Stanley Dean Witter & Co       ...         No   \n",
      "8           Banc of America Securities LLC\\nUBS Warburg       ...         No   \n",
      "9                                   Morgan Stanley & Co       ...         No   \n",
      "10                               HC Wainwright & Co Inc       ...         No   \n",
      "11                                      Van Kasper & Co       ...         No   \n",
      "12                                Bear Stearns & Co Inc       ...        Yes   \n",
      "13                                      Lehman Brothers       ...         No   \n",
      "14     JP Morgan & Co Inc\\nCitigroup Global Markets Inc       ...        Yes   \n",
      "15    Morgan Stanley & Co\\nBank of America Merrill L...       ...        Yes   \n",
      "16                       Friedman Billings Ramsey Group       ...         No   \n",
      "17                                           Cowen & Co       ...        Yes   \n",
      "18                                   Goldman Sachs & Co       ...         No   \n",
      "19                   BancAmerica Robertson Stephens Inc       ...        Yes   \n",
      "20              Kashner Davidson Securities Corporation       ...         No   \n",
      "21                                     CIBC Oppenheimer       ...        Yes   \n",
      "22                       Banc of America Securities LLC       ...        Yes   \n",
      "23                                 CS First Boston Corp       ...        Yes   \n",
      "24                                           Cowen & Co       ...         No   \n",
      "25                                   Jefferies & Co Inc       ...        Yes   \n",
      "26                                           Cowen & Co       ...        Yes   \n",
      "27     Citigroup Global Markets Inc\\nLeerink Swann & Co       ...        Yes   \n",
      "28                                Hambrecht & Quist Inc       ...         No   \n",
      "29                    BancBoston Robertson Stephens Inc       ...        Yes   \n",
      "...                                                 ...       ...        ...   \n",
      "2970                               CS First Boston Corp       ...        Yes   \n",
      "2971                    Morgan Stanley Dean Witter & Co       ...        Yes   \n",
      "2972  Stephens Inc\\nKeefe Bruyette & Woods Inc\\nRobe...       ...         No   \n",
      "2973                              Alex Brown & Sons Inc       ...        Yes   \n",
      "2974  JP Morgan & Co Inc\\nCredit Suisse Securities (...       ...        Yes   \n",
      "2975                                               Citi       ...        Yes   \n",
      "2976                              Bear Stearns & Co Inc       ...        Yes   \n",
      "2977                Lehman Brothers\\nJP Morgan & Co Inc       ...         No   \n",
      "2978                                Cruttenden Roth Inc       ...         No   \n",
      "2979                                     Morgan Stanley       ...        Yes   \n",
      "2980                                CE Unterberg Towbin       ...         No   \n",
      "2981  JP Morgan & Co Inc\\nDeutsche Bank Securities C...       ...        Yes   \n",
      "2982                                 JP Morgan & Co Inc       ...         No   \n",
      "2983  RBC Capital Markets\\nBarclays\\nStifel Nicolaus...       ...        Yes   \n",
      "2984                    Credit Suisse First Boston Corp       ...         No   \n",
      "2985            Goldman Sachs & Co\\nMorgan Stanley & Co       ...        Yes   \n",
      "2986  Bank of America Merrill Lynch\\nCitigroup Globa...       ...        Yes   \n",
      "2987                             Merrill Lynch & Co Inc       ...         No   \n",
      "2988                                    Lehman Brothers       ...        Yes   \n",
      "2989                               CS First Boston Corp       ...        Yes   \n",
      "2990  Goldman Sachs & Co\\nCredit Suisse\\nJP Morgan &...       ...         No   \n",
      "2991  Deutsche Bank Securities Inc\\nCitigroup\\nWells...       ...         No   \n",
      "2992  Sandler O'Neill Partners  L.P.\\nKeefe Bruyette...       ...         No   \n",
      "2993                                   MDB Capital Corp       ...         No   \n",
      "2994  Robert W Baird & Co Inc\\nBMO Capital Markets\\n...       ...         No   \n",
      "2995      BMO Capital Markets\\nWedbush Securities, Inc.       ...        Yes   \n",
      "2996  Merrill Lynch Pierce Fenner & Smith\\nJefferies...       ...        Yes   \n",
      "2997                                   MDB Capital Corp       ...         No   \n",
      "2998                          Jefferies LLC\\nCowen & Co       ...        Yes   \n",
      "2999            Citi\\nJP Morgan & Co Inc\\nCredit Suisse       ...         No   \n",
      "\n",
      "     min_round_vexp avg_round_vexp max_round_vexp min_firm_amt_vexp  \\\n",
      "0               NaN            NaN            NaN               NaN   \n",
      "1          60'000.0      108'666.7      140'000.0          35'000.0   \n",
      "2               NaN            NaN            NaN               NaN   \n",
      "3               NaN            NaN            NaN               NaN   \n",
      "4               NaN            NaN            NaN               NaN   \n",
      "5               NaN            NaN            NaN               NaN   \n",
      "6               NaN            NaN            NaN               NaN   \n",
      "7               NaN            NaN            NaN               NaN   \n",
      "8           2'999.0          999.7        2'999.0             999.0   \n",
      "9          64'480.0       64'480.0       64'480.0          64'480.0   \n",
      "10              NaN            NaN            NaN               NaN   \n",
      "11              NaN            NaN            NaN               NaN   \n",
      "12          4'500.0        8'083.5       11'667.0           4'500.0   \n",
      "13              NaN            NaN            NaN               NaN   \n",
      "14            740.0       25'149.8       76'278.8             750.0   \n",
      "15          7'000.0       23'800.0       50'000.0           1'800.0   \n",
      "16        175'000.0      175'000.0      175'000.0           4'000.0   \n",
      "17            360.0        4'548.1       10'000.0           1'800.0   \n",
      "18              NaN            NaN            NaN               NaN   \n",
      "19          1'612.0        5'570.7       10'000.0           1'612.0   \n",
      "20              NaN            NaN            NaN               NaN   \n",
      "21            410.0        3'828.4       12'000.0             820.0   \n",
      "22          3'000.0       14'075.1       36'000.0           1'785.7   \n",
      "23          5'000.0       15'948.1       38'500.0              54.7   \n",
      "24              NaN            NaN            NaN               NaN   \n",
      "25          1'783.0        6'125.4       21'706.0           4'289.0   \n",
      "26          3'300.0        9'161.8       23'400.0           1'000.0   \n",
      "27          1'895.0       16'605.8       31'111.0           1'809.9   \n",
      "28            302.0       15'574.3       37'085.0          46'723.0   \n",
      "29            520.0        2'513.0        5'003.0              63.0   \n",
      "...             ...            ...            ...               ...   \n",
      "2970        2'000.0       22'680.0       50'000.0             195.0   \n",
      "2971          500.0      100'626.7      191'500.0           1'200.0   \n",
      "2972       50'000.0       50'000.0       50'000.0          50'000.0   \n",
      "2973            NaN            NaN            NaN               NaN   \n",
      "2974            NaN            NaN            NaN               NaN   \n",
      "2975          565.0       11'150.6       30'500.0             210.0   \n",
      "2976        4'500.0      103'306.3      608'838.0             999.0   \n",
      "2977            NaN            NaN            NaN               NaN   \n",
      "2978            NaN            NaN            NaN               NaN   \n",
      "2979       12'000.1       27'175.1       62'400.0           2'337.5   \n",
      "2980            NaN            NaN            NaN               NaN   \n",
      "2981        5'700.0        7'675.0       15'000.0              15.0   \n",
      "2982            NaN            NaN            NaN               NaN   \n",
      "2983        9'095.0       10'365.0       22'000.0           4'547.5   \n",
      "2984            NaN            NaN            NaN               NaN   \n",
      "2985        3'000.0       13'386.9       40'000.0           1'263.6   \n",
      "2986            NaN            NaN            NaN               NaN   \n",
      "2987            2.0            1.0            2.0               2.0   \n",
      "2988        1'260.0       21'074.6       62'720.0           1'700.0   \n",
      "2989          200.0        6'451.6       16'000.0              20.0   \n",
      "2990       34'750.0       11'583.3       34'750.0          34'750.0   \n",
      "2991            NaN            NaN            NaN               NaN   \n",
      "2992        1'931.0       18'990.3       46'569.0           5'000.0   \n",
      "2993            NaN            NaN            NaN               NaN   \n",
      "2994            NaN            NaN            NaN               NaN   \n",
      "2995           41.0       11'323.9       50'000.0             612.5   \n",
      "2996            NaN            NaN            NaN               NaN   \n",
      "2997            NaN            NaN            NaN               NaN   \n",
      "2998        2'801.0        9'757.8       18'700.0           1'369.0   \n",
      "2999       10'000.0       10'000.0       10'000.0          10'000.0   \n",
      "\n",
      "     avg_firm_amt_vexp max_firm_amt_vexp min_fund_amt_vexp max_fund_amt_vexp  \\\n",
      "0                  NaN               NaN               NaN               NaN   \n",
      "1             65'200.0         105'000.0          35'000.0         105'000.0   \n",
      "2                  NaN               NaN               NaN               NaN   \n",
      "3                  NaN               NaN               NaN               NaN   \n",
      "4                  NaN               NaN               NaN               NaN   \n",
      "5                  NaN               NaN               NaN               NaN   \n",
      "6                  NaN               NaN               NaN               NaN   \n",
      "7                  NaN               NaN               NaN               NaN   \n",
      "8                749.8           2'000.0             999.0           2'000.0   \n",
      "9             64'480.0          64'480.0          64'480.0          64'480.0   \n",
      "10                 NaN               NaN               NaN               NaN   \n",
      "11                 NaN               NaN               NaN               NaN   \n",
      "12             8'083.5          11'667.0           4'000.0           7'667.0   \n",
      "13                 NaN               NaN               NaN               NaN   \n",
      "14            12'574.9          55'940.6               7.0          18'507.6   \n",
      "15            14'875.0          80'000.0           1'800.0          80'000.0   \n",
      "16            87'500.0         171'000.0           4'000.0         171'000.0   \n",
      "17             4'548.1          10'000.0             535.0          10'000.0   \n",
      "18                 NaN               NaN               NaN               NaN   \n",
      "19             4'178.0          10'000.0           1'612.0          10'000.0   \n",
      "20                 NaN               NaN               NaN               NaN   \n",
      "21             2'871.3           4'993.4             400.0           2'200.0   \n",
      "22             8'445.1          32'000.0           1'785.7          32'000.0   \n",
      "23             6'379.3          33'000.0              54.7          12'800.0   \n",
      "24                 NaN               NaN               NaN               NaN   \n",
      "25            14'292.7          38'589.0           4'289.0           4'289.0   \n",
      "26             4'580.9          22'644.0              49.0          22'644.0   \n",
      "27            12'177.6          27'296.9             668.0          24'001.0   \n",
      "28            46'723.0          46'723.0          46'723.0          46'723.0   \n",
      "29             1'884.8           5'375.0               3.0           1'750.0   \n",
      "...                ...               ...               ...               ...   \n",
      "2970           8'100.0          19'158.0             195.0          19'158.0   \n",
      "2971          37'735.0         198'373.5           1'200.0         156'600.0   \n",
      "2972          50'000.0          50'000.0          25'000.0          25'000.0   \n",
      "2973               NaN               NaN               NaN               NaN   \n",
      "2974               NaN               NaN               NaN               NaN   \n",
      "2975           9'911.7          31'675.6             100.0           6'032.6   \n",
      "2976          77'479.8         605'841.0              15.0           2'499.0   \n",
      "2977               NaN               NaN               NaN               NaN   \n",
      "2978               NaN               NaN               NaN               NaN   \n",
      "2979          13'587.5          61'859.0             335.7          61'859.0   \n",
      "2980               NaN               NaN               NaN               NaN   \n",
      "2981           4'385.7           8'978.3              15.0           7'083.3   \n",
      "2982               NaN               NaN               NaN               NaN   \n",
      "2983          10'365.0          26'547.5           4'547.5          26'547.5   \n",
      "2984               NaN               NaN               NaN               NaN   \n",
      "2985           7'649.7          17'051.1           1'263.6           8'179.0   \n",
      "2986               NaN               NaN               NaN               NaN   \n",
      "2987               2.0               2.0               2.0               2.0   \n",
      "2988          18'733.0          51'250.0             701.0          51'250.0   \n",
      "2989           3'225.8           8'043.0              20.0           7'173.0   \n",
      "2990          11'583.3          34'750.0          34'750.0          34'750.0   \n",
      "2991               NaN               NaN               NaN               NaN   \n",
      "2992          37'980.5          70'961.0           5'000.0          70'961.0   \n",
      "2993               NaN               NaN               NaN               NaN   \n",
      "2994               NaN               NaN               NaN               NaN   \n",
      "2995          11'323.9          33'801.8              41.0          24'066.0   \n",
      "2996               NaN               NaN               NaN               NaN   \n",
      "2997               NaN               NaN               NaN               NaN   \n",
      "2998           7'806.2          19'648.0             100.0           4'500.0   \n",
      "2999          10'000.0          10'000.0          10'000.0          10'000.0   \n",
      "\n",
      "     description_cat  \n",
      "0      Common Shares  \n",
      "1      Common Shares  \n",
      "2      Common Shares  \n",
      "3      Common Shares  \n",
      "4      Common Shares  \n",
      "5      Common Shares  \n",
      "6      Common Shares  \n",
      "7      Common Shares  \n",
      "8      Common Shares  \n",
      "9     Class A Shares  \n",
      "10     Common Shares  \n",
      "11     Common Shares  \n",
      "12     Common Shares  \n",
      "13     Common Shares  \n",
      "14     Common Shares  \n",
      "15     Common Shares  \n",
      "16     Common Shares  \n",
      "17     Common Shares  \n",
      "18    Class A Shares  \n",
      "19     Common Shares  \n",
      "20     Common Shares  \n",
      "21     Common Shares  \n",
      "22     Common Shares  \n",
      "23     Common Shares  \n",
      "24     Common Shares  \n",
      "25     Common Shares  \n",
      "26     Common Shares  \n",
      "27     Common Shares  \n",
      "28     Common Shares  \n",
      "29     Common Shares  \n",
      "...              ...  \n",
      "2970   Common Shares  \n",
      "2971  Class A Shares  \n",
      "2972   Common Shares  \n",
      "2973   Common Shares  \n",
      "2974   Common Shares  \n",
      "2975   Common Shares  \n",
      "2976   Common Shares  \n",
      "2977  Class A Shares  \n",
      "2978   Common Shares  \n",
      "2979   Common Shares  \n",
      "2980   Common Shares  \n",
      "2981   Common Shares  \n",
      "2982   Common Shares  \n",
      "2983   Common Shares  \n",
      "2984   Common Shares  \n",
      "2985   Common Shares  \n",
      "2986   Common Shares  \n",
      "2987   Common Shares  \n",
      "2988   Common Shares  \n",
      "2989   Common Shares  \n",
      "2990   Common Shares  \n",
      "2991  Class A Shares  \n",
      "2992  Class A Shares  \n",
      "2993   Common Shares  \n",
      "2994   Common Shares  \n",
      "2995   Common Shares  \n",
      "2996   Common Shares  \n",
      "2997   Common Shares  \n",
      "2998   Common Shares  \n",
      "2999   Common Shares  \n",
      "\n",
      "[3000 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#Devide dataframe in function of type.\n",
    "ints = ipo.select_dtypes(include='int64')\n",
    "floats = ipo.select_dtypes(include='float64')\n",
    "objects = ipo.select_dtypes(include='object')\n",
    "timestamps = ipo.select_dtypes(include='M8[ns]')\"\"\"\n",
    "\n",
    "print(ipo_processing.select_dtypes(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-23048729d7e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mipo_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     corr_dic[column] = abs(ipo_processing[column]\n\u001b[1;32m----> 5\u001b[1;33m                            .corr(ipo_processing['Target']))\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorr_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2698\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2700\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target'"
     ]
    }
   ],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_dic = {}\n",
    "for column in ipo_processing.columns:\n",
    "    corr_dic[column] = abs(ipo_processing[column]\n",
    "                           .corr(ipo_processing['Target']))\n",
    "\n",
    "for w in sorted(corr_dic, key=corr_dic.get, reverse=True):\n",
    "    print (w, corr_dic[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.7.2. Check the correlation between feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(dataframe.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _________________________________________________________ \n",
    "## Part 1\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using non-text fields. By non-text fields, we mean all fields except the 'Risk_Factors'. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(non-text-fields) -> Probability of being in class 1 \n",
    "\n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit peut être mettre tous les modèles dans un autre notebook si on va les utiliser dans les autres parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## J'enlève les dates en attendant\n",
    "ipo_processing.drop( columns = ['lockup_date','lockup_days','Closing_Price','Offering_Price','first_trade_date','offer_date','issue_date'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ipo_processing['Target'] \n",
    "features = ipo_processing.drop(['Risk_Factors','Target'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate target and features into test and training and validation sets\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = seed)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "bbaseline_clf = DummyClassifier(strategy='stratified', random_state = seed)\n",
    "\n",
    "# Fit the dummy classifier \n",
    "bbaseline_clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict target probabilities of belonging to positive class\n",
    "y_pred = bbaseline_clf.predict_proba(X_test)\n",
    "\n",
    "# Compute area under the curve score\n",
    "print('auc',roc_auc_score(y_test, y_pred[:,1]))\n",
    "\n",
    "plot_roc_curve('Dummy',y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_clf = LogisticRegression() \n",
    "baseline_clf.fit(X_train,y_train)\n",
    "y_pred = baseline_clf.predict_proba(X_test)\n",
    "print('Score ',roc_auc_score(y_test, y_pred[:,1]))\n",
    "plot_roc_curve('Logistic Regression',y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression tuning c penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standardize features and classifier in a single pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lr_clf', LogisticRegression()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(lr_clf__penalty='l1')\n",
    "\n",
    "# Finding best value of C using validation set\n",
    "scores = []\n",
    "Cs = []\n",
    "for C in np.logspace(-4, 5, 10):\n",
    "    pipeline.set_params(lr_clf__C=C) \n",
    "    pipeline.fit(X_train,y_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_test)\n",
    "    scores.append(roc_auc_score(y_test, y_train_pred[:,1]))\n",
    "    Cs.append(C)\n",
    "\n",
    "best_C = Cs[scores.index(max(scores))]\n",
    "print ('best C = %d with auc score = %2.4f' %(best_C, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(lr_clf__C=best_C)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_lr = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_lr[:,1])\n",
    "print ('lr classifer auc with l1 regularization = %2.4f' %score)\n",
    "plot_roc_curve('l1 regularization',y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN tuning N neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features and classifier in a single pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('knn_clf', KNeighborsClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Finding best value of K using validation set\n",
    "scores = []\n",
    "Ks = []\n",
    "for K in [int(i) for i in np.linspace(5, 95, 10)]:\n",
    "    pipeline.set_params(knn_clf__n_neighbors = K) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    Ks.append(K)\n",
    "\n",
    "best_K = Ks[scores.index(max(scores))]\n",
    "print ('best K = %d with auc score = %2.4f' %(best_K, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(knn_clf__n_neighbors = best_K)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_knn = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_knn[:,1])\n",
    "print ('knn classifer auc = %2.4f' %score)\n",
    "plot_roc_curve('KNN',y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define a random classifier pipeline\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(rf_clf__random_state = seed)\n",
    "    \n",
    "# Finding best value of n_estimators using validation set\n",
    "scores = []\n",
    "NSs = []\n",
    "for NS in [int(i) for i in np.linspace(10, 100, 10)]:\n",
    "    pipeline.set_params(rf_clf__n_estimators = NS) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    NSs.append(NS)\n",
    "\n",
    "best_NS = NSs[scores.index(max(scores))]\n",
    "print ('best NS = %d with auc score = %2.4f' %(best_NS, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(rf_clf__n_estimators = best_NS)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_rf = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_rf[:,1])\n",
    "print ('rf classifer auc = %2.4f' %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve('KNN',y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define a random classifier pipeline\n",
    "estimators = []\n",
    "estimators.append(('gb_clf', GradientBoostingClassifier()))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.set_params(gb_clf__random_state = seed)\n",
    "    \n",
    "# Finding best value of n_estimators using validation set\n",
    "scores = []\n",
    "NSs = []\n",
    "for NS in [int(i) for i in np.linspace(10, 100, 10)]:\n",
    "    pipeline.set_params(gb_clf__n_estimators = NS) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)\n",
    "    scores.append(roc_auc_score(y_train_val, y_train_pred[:,1]))\n",
    "    NSs.append(NS)\n",
    "\n",
    "best_NS = NSs[scores.index(max(scores))]\n",
    "print ('best NS = %d with auc score = %2.4f' %(best_NS, max(scores)))\n",
    "\n",
    "# Performance of the tuned model on test set\n",
    "pipeline.set_params(gb_clf__n_estimators = best_NS)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred_gb = pipeline.predict_proba(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_gb[:,1])\n",
    "plot_roc_curve('GradientBoostingClassifier',y_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using __only__ textual field 'Risk_Factors'. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(text-fields) -> Probability of being in class 1 \n",
    "\n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "def nested_cv(X, y, est_pipe, p_grid, p_score, n_splits_inner = 3, n_splits_outer = 3, n_cores = 1, seed = 0):\n",
    "\n",
    "    # Cross-validation schema for inner and outer loops (stratified if it is a classification)\n",
    "    inner_cv = KFold(n_splits = n_splits_inner, shuffle = True, random_state = seed)\n",
    "    outer_cv = KFold(n_splits = n_splits_outer, shuffle = True, random_state = seed)\n",
    "    \n",
    "    # Grid search to tune hyper parameters\n",
    "    est = GridSearchCV(estimator = est_pipe, param_grid = p_grid, cv = inner_cv, scoring = p_score, n_jobs = n_cores)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_scores = cross_val_score(estimator = est, X = X, y = y, cv = outer_cv, scoring = p_score, n_jobs = n_cores)\n",
    "    \n",
    "    print('Average score: %0.4f (+/- %0.4f)' % (nested_scores.mean(), nested_scores.std() * 1.96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of Risk Factors :  risks_tfidf    \n",
    "features = risks_tfidf\n",
    "seed = 0\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regroupe to 50 Fields \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "\n",
    "principalComponents = pca.fit_transform(risks_tfidf)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "\n",
    "\n",
    "\n",
    "#Update the name of the columns\n",
    "# get length of df's columns\n",
    "num_cols = 100\n",
    "# generate range of ints for suffixes\n",
    "rng = range(0,num_cols)\n",
    "\n",
    "new_cols = [ 'risk_'+str(i) for i in rng]\n",
    "\n",
    "# ensure the length of the new columns list is equal to the length of df's columns\n",
    "principalComponents.columns = new_cols[:num_cols]\n",
    "\n",
    "principalComponents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = principalComponents\n",
    "seed = 0\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in [10, 20, 50, 100]]}\n",
    "\n",
    "nested_cv(X = features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word Vectors : Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk.data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def review_to_wordlist( review ):\n",
    "    \n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "   \n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences, where each sentence is a word list\n",
    "\n",
    "def review_to_sentences( review, tokenizer ):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())  \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:      \n",
    "        if len(raw_sentence) > 0:           \n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "   \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = []\n",
    "risks = ipo['Risk_Factors']\n",
    "for risk in risks:\n",
    "    sentences += review_to_sentences(risk, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word vectors\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # word vector dimensionality                      \n",
    "min_word_count = 40   # minimum word count                        \n",
    "num_workers = 16      # number of threads to run in parallel\n",
    "context = 10          # context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model \n",
    "from gensim.models import word2vec\n",
    "print ('Training model...')\n",
    "w2v_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "print ('Done !')\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient\n",
    "w2v_model.init_sims(replace=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all of the word vectors in a given paragraph\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0.\n",
    "     \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Given a set of reviews (each one a list of words), calculate \n",
    "# the average feature vector for each one and return a 2D numpy array\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype='float32')\n",
    "     \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000 == 0:\n",
    "           print ('Review %d of %d' % (counter, len(reviews)))\n",
    "       \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature vectors for review data,\n",
    "# using the functions we defined above.\n",
    "\n",
    "clean_data_reviews = []\n",
    "for review in ipo['Risk_Factors']:\n",
    "    clean_data_reviews.append( review_to_wordlist( review ))\n",
    "\n",
    "w2v_features = getAvgFeatureVecs( clean_data_reviews, w2v_model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Random Forest with Average Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 0\n",
    "\n",
    "# Define pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf_clf', RandomForestClassifier()))\n",
    "rf_pipe = Pipeline(estimators)\n",
    "rf_pipe.set_params(rf_clf__random_state = seed)\n",
    "\n",
    "# Fixed parameters\n",
    "score = 'accuracy'\n",
    "\n",
    "# Setup possible values of parameters to optimize over\n",
    "p_grid = {\"rf_clf__n_estimators\": [int(i) for i in np.linspace(10.0, 50.0, 5)]}\n",
    "\n",
    "nested_cv(X = w2v_features, y = target, est_pipe = rf_pipe, p_grid = p_grid, p_score = score, n_cores = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph vectors : Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec needs each review to be tagged with some sort of ids\n",
    "# Here we tag each review with the 'id' field\n",
    "\n",
    "import gensim\n",
    "\n",
    "tagged_clean_data_reviews = []\n",
    "for uid, review in zip(data['id'], clean_data_reviews):\n",
    "    tagged_clean_data_reviews.append(gensim.models.doc2vec.TaggedDocument(words=review, tags=['%s' % uid[1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Predict whether the closing price is higher than the offering price using __all__ fields. If the price goes up from opening to closing, assign a value of 1 to a new target variable called __Price_Increase__, otherwise assign 0.\n",
    "\n",
    "    f(all-fields) -> Probability of being in class 1 \n",
    "    \n",
    "For the evaluation metric, report the area under the curve (AUC) and plot an ROC graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "  \n",
    "Predict the share price at the end of the day using __all__ fields.\n",
    "\n",
    "    f(all-fields) ->  Share price at the end of the first day of trading\n",
    "    \n",
    "For the evaluation metric, report statisitcs for R-squared, Residual Mean Squared Error, Mean Absolute Error, and Median Absolute Error; however, be sure to tune and hypertune your models using R-Squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "As mentioned earlier, you can find the dataset under project directory in the course git repository under the name of *ipo.xlsx*. The description of each variable can also be found in *variable_description.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "We expect your solution for each step to contain the following:\n",
    "\n",
    "* data preprocessing and feature extraction (can be shared across different steps)\n",
    "* feature reduction\n",
    "* train, tune and test different predictive models\n",
    "* model comparison and arguing about the best model (don't forget mentioning a baseline model)\n",
    "* predict the labels of the to_predict dataset using your final model\n",
    "* discussion on possible additional tasks that can be done to boost the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "* Predict values from your *best* predictive model for the target variable in Parts 1 to 4 above, and insert those values into the file __*ipo_to_predict.xlsx*__ The fields to be completed by you are: __Price_Change_Non_Textual__ (Part 1), __Price_Change_Textual__ (Part 2), __Price_Change_All__ (Part 3), and __Price_All__ (Part 4).  \n",
    "  \n",
    "  \n",
    "* Deliver a Jupyter notebook with an explanation of your methods, codes and results. Don't forget to divide your notebook into different parts, which clearly shows your solution to the common pre-processing as well as different steps separately. \n",
    "    \n",
    "    \n",
    "\n",
    "* Submit your final notebook and files into the git repository of the team (we will create that git repo for you).\n",
    "\n",
    "\n",
    "* Present your results in the final session of the course. Communicate them in a clear and concise manner. The goal is to learn how to present your results to stakeholders at the right level of detail. **We will discuss this more in classe**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "* Take some time at the start of the project to educate yourslef about the IPO process. We privde you wiht two main texts in the class repository under *resources* folder. Understanding how variables relate to the target outcomes will help you to construct new measures from the tabularized data and/or selecting or eliminating features that relate to the target variable.  \n",
    "\n",
    "\n",
    "* Present your results as a story - this is very important!   \n",
    "  \n",
    "\n",
    "* Document all of your assumptions (e.g. evaluation metric, hyper-parameter values, ...).  \n",
    "\n",
    "\n",
    "* Make sure your code will run and results are reproducible (fix random seeds, etc.).  \n",
    "\n",
    "\n",
    "*  Comment your blocks of code (and lines of code if needed) and anything in your story/logic that might not be obvious by looking at your code.    \n",
    "\n",
    "\n",
    "* To speed up experimentation, you might use a small sample of the original dataset to do your initial coding. Also try to use all possible cores for computation, by setting the option of n_jobs = -1, when needed. \n",
    "\n",
    "\n",
    "* Try to be creative to improve your predictions, but don't forget that it is also important to explain your line of thinking/reasoning.\n",
    "\n",
    "\n",
    "* Your final grade is based on the whole process of doing the project and not just based on your results on the unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Grading of the project (apart from presentation), is based on the following components:\n",
    "    \n",
    "    20 %  Documentation and organization of your notebook\n",
    "    15 %  Quality and commenting of code\n",
    "    10 %  Pre-processing\n",
    "    15 %  Part 1\n",
    "    15 %  Part 2\n",
    "    10 %  Part 3\n",
    "    15 %  Part 4\n",
    "    \n",
    "     5%   Bonus Contest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional bonus contest at the end of the project, we will award an extra 5% of the total project grade to the team that comes up with the \"best\" strategy for investing into IPOs based on your estimated model(s). Specifically, assume you have USD 1,000,000 to invest into the IPO stocks that appear in the \"unseen\" file __*ipo_to_predict.xlsx*__. In the column \"Your_Bet\", allocate some portion of that USD 1,000,000 to each of the stocks listed in the unseen file. The total allocation must sum to $1,000,000. The top team making the most money (once outcomes are revealed at time of grading) will earn the 5% bonus. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
